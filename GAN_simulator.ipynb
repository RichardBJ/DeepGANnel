{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "V_sgB_5dx1f1"
   },
   "outputs": [],
   "source": [
    "#Please cite us if you find this code useful:\n",
    "#Currently under peer review, but this doi: pre-print is live.\n",
    "#https://doi.org/10.1101/2020.06.25.171918    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rF2x3qooyBTI"
   },
   "source": [
    "# DeepGANnel A Convolutional Generative Adversarial Network to generate labelled timeseries physiological data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "### Import libraries  Tested with TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5oue0oqCkZZ"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g5RstiiB8V-z"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "print(\"to here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZKbyU2-AiY-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wx-zNbLqB4K8"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from IPython import display\n",
    "import sys\n",
    "from scipy.spatial.distance import euclidean\n",
    "from fastdtw import fastdtw\n",
    "import random as rd\n",
    "from datetime import datetime\n",
    "import csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use too many files it can \"confuse\" the network and lead to the generation of average data\n",
    "files=[\"claire_035[NF=50Hz]0-32assembled.csv\"]\n",
    "\n",
    "seq=1280\n",
    "jumps=seq\n",
    "limit=5000000000000000\n",
    "\n",
    "sam=False #are we importing real or synthetic data from Sam.\n",
    "CARTOON=True # draw a cartoon?\n",
    "baselinecorrect=False\n",
    "scamp=10\n",
    "flip=True # simple augmentation parameters\n",
    "masternoise=1.0\n",
    "SD=[]\n",
    "\n",
    "#plot\n",
    "start=15000\n",
    "end=25000\n",
    "\n",
    "def mean(list):\n",
    "    return(sum(list)/len(list))\n",
    "\n",
    "print('Parsing data...')\n",
    "def dofile(newfile):\n",
    "    print(newfile)\n",
    "    i=0\n",
    "    j=0\n",
    "    firstline=True\n",
    "    x=[]\n",
    "    y=[]\n",
    "    with open(newfile) as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        for row in reader:\n",
    "\n",
    "            if firstline==True:\n",
    "                # Don't read first line if header exists\n",
    "                firstline=False\n",
    "                continue\n",
    "\n",
    "            elif i > limit:\n",
    "                # Stop if past limit\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                # Add current and channels to respective lists\n",
    "                i += 1\n",
    "                try:\n",
    "                    x.append(float(row[1]))\n",
    "                except:\n",
    "                    print(\"An exception occurred\",print(row[1]))\n",
    "                    break\n",
    "                y.append(float(row[2]))\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                # Progress update every 10000 iterations\n",
    "                print(f'Progress: {i}/{limit}', end='\\r')\n",
    "                sys.stdout.flush()\n",
    "        #make sure y is zero or 1\n",
    "        miny=min(y)\n",
    "        maxy=max(y)\n",
    "        print('max and mins of labels',maxy, miny)\n",
    "        zy=[]\n",
    "        for val in y:\n",
    "            if val == miny:\n",
    "                zy.append(0)\n",
    "            else:\n",
    "                zy.append(1)\n",
    "        return x,zy\n",
    "x,y = [],[]\n",
    "for filename in files:\n",
    "    newx,newy=dofile(filename)\n",
    "    print('\\nnewx len = \\n',np.asarray(newx).shape)\n",
    "    x.extend(newx)\n",
    "    y.extend(newy)\n",
    "    print('x shape, y shape',np.asarray(x).shape,np.asarray(y).shape)\n",
    "    \n",
    "print(\"\\nlength of lists=\\n\")         \n",
    "len(x)\n",
    "num=len(x) # Number of samples in x and y\n",
    "\n",
    "#reverse the lists and concatenate\n",
    "superx=[val for val in x]\n",
    "supery=[val for val in y]\n",
    "x.reverse()\n",
    "y.reverse()\n",
    "superx.extend([val for val in x])\n",
    "supery.extend([val for val in y])\n",
    "x=superx\n",
    "y=supery\n",
    "print(\"\\nlength of lists now=\\\\\")    \n",
    "print('x shape',np.asarray(x).shape)\n",
    "num=len(superx) # Number of samples in x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some scaling\n",
    "x=np.asarray(x)\n",
    "y=np.asarray(y)\n",
    "\n",
    "maxx=np.max(x)\n",
    "maxy=np.max(y)\n",
    "miny=np.min(y)\n",
    "minx=np.min(x)\n",
    "maxer=max([maxx,maxy])\n",
    "miner=max([minx,miny])\n",
    "\n",
    "\n",
    "print(\"original global max and mins\", maxer,miner)\n",
    "\n",
    "y=y*2\n",
    "y=y-1\n",
    "\n",
    "x=x-miner\n",
    "x=x/(maxer)-0.5\n",
    "\n",
    "print(\"\\nx\",np.min(x),np.max(x),end=\"\\n\")\n",
    "print(\"\\ny\",np.min(y),np.max(y),end=\"\\n\")\n",
    "#should now be +/- 1 or less!!!\n",
    "print('Converting to correct shape...')\n",
    "'''offset x!!'''\n",
    "from scipy.stats import mode\n",
    "x=x.tolist()\n",
    "y=y.tolist()\n",
    "vals=[val for val in x if val < 0]\n",
    "mode,count= mode(vals)\n",
    "x=[val-mode for val in x]\n",
    "del vals\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(x[start:end])\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(y[start:end])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def noiseup(listin,label,numberofcopies,noiselev):\n",
    "    # We start by creating a large augmented dataset based on the original\n",
    "    # The examples will be inperceptable different to each other (by human eye)\n",
    "    nc=numberofcopies\n",
    "    copies=[el for el in listin]\n",
    "    l_label=[el for el in label]\n",
    "    signal=np.asarray(listin)\n",
    "    for k in range(nc):\n",
    "        sd=rd.uniform(noiselev/2, noiselev*2)\n",
    "        print(\"noise loop:{:} = {:} \".format(k,sd))\n",
    "        noise = np.random.normal(0, sd, signal.shape)\n",
    "        copies.extend(list(signal + noise))\n",
    "        #must duplicated y too!\n",
    "        label.extend(l_label)\n",
    "        start=15000\n",
    "        end=25000\n",
    "    return copies, label\n",
    "\n",
    "#make 10 noise copies\n",
    "x,y=noiseup(x,y,200,0.003)\n",
    "print(\"\\nlength of lists post noising=\\n\")         \n",
    "print(len(x),len(y))\n",
    "\n",
    "newx=[el for el in x]\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(x[start:end])\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(y[start:end])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeShape2c(x, y, seq, num, i):\n",
    "    z = np.ones((2,seq,1))\n",
    "    z[0, 0:seq, 0] = x[i: i+seq]\n",
    "    z[1, 0:seq, 0] = y[i: i+seq]\n",
    "    \n",
    "    if i % (seq * 10000) == 0:\n",
    "        print(f'Progress: Batch {i // seq}/{num // seq}', end='\\r')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return z\n",
    "\n",
    "'''Now draw the labels and data out\n",
    "note 2c is just a code for the version of this function we used'''\n",
    "mtrain_images=[makeShape2c(x, y, seq, num, i) for i in range(0, num-seq, jumps)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To balnce the data set (we don't want pages of nothing) only keep those with between f1 and f2 zeros... eg, 0.1, 0.9\n",
    "# We will treat LIKE images, but of course \"image\" has no meaning, it is just an array\n",
    "if 'x' in locals() or 'y' in globals():\n",
    "    del x\n",
    "    del y\n",
    "pmin=0.1\n",
    "pmax=0.9\n",
    "better=[]\n",
    "print(\"images.shape\",np.asarray(mtrain_images).shape)\n",
    "print(\"Now deleting boring images\")\n",
    "for image in mtrain_images:\n",
    "    lmin=min(image[1,:,0])\n",
    "    array=np.asarray(image)\n",
    "    mins=array[1,array[1,:,0]==lmin,0] #how many zeros\n",
    "    z=len(list(mins))\n",
    "    p=z/len(image[1,:,0])\n",
    "    if p>0.1 and p<0.9:\n",
    "        better.append(image)\n",
    "        \n",
    "    #if max(image[1,:,0])!=min(image[1,:,0]):\n",
    "        #better.append(image)''\n",
    "mtrain_images=better\n",
    "print(\"images.shape\",np.asarray(mtrain_images).shape)\n",
    "        \n",
    "\n",
    "print('\\nConverting to numpy array')\n",
    "mtrain_images=np.asarray(mtrain_images)\n",
    "\n",
    "sys.stdout.flush()\n",
    "print(f'\\n{mtrain_images.shape}\\n')\n",
    "print('finito')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFC2ghIdiZYE"
   },
   "outputs": [],
   "source": [
    "#print(len(x),len(y))\n",
    "mtrain_images = mtrain_images.reshape(mtrain_images.shape[0],2,seq,1)\n",
    "print(mtrain_images.shape, np.min(mtrain_images),np.max(mtrain_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4PIDhoDLbsZ"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = mtrain_images.shape[0]\n",
    "BATCH_SIZE = 128\n",
    "looser=[]\n",
    "glooser=[]\n",
    "ms=[]\n",
    "dtw=[]\n",
    "xout=[] \n",
    "yout=[]\n",
    "ep=[]\n",
    "EPOCHS = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yKCCQOoJ7cn"
   },
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(mtrain_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Two models are needed, one Generator and one Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-tEyxE-GMC48"
   },
   "source": [
    "## The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6bpTcDqoLWjY"
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    # Up scale through each layer, starting with a small strip of 2xX data\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(2*10*64, use_bias=False, input_shape=(2*seq,),dtype='float32'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((2, 10, 64)))\n",
    "    assert model.output_shape == (None, 2, 10, 64) # Bbatch size is \"None\"\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (2, 5), strides=(1, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 2, 20, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())                                                                                                      \n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(64, (2, 5), strides=(1, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 2, 40, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(32, (2, 5), strides=(1, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 2, 80, 32)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU()) \n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(32, (2, 5), strides=(1, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 2, 160, 32)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())          \n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(32, (2, 50), strides=(1, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 2, 320, 32)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Conv2DTranspose(32, (2, 5), strides=(1, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 2, 640, 32)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())        \n",
    "       \n",
    "    model.add(layers.Conv2DTranspose(1, (2, 5), strides=(1, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 2, seq, 1)\n",
    "    model.summary()\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gl7jcC7TdPTG"
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random block of data the right shape\n",
    "noise = tf.random.normal([1, 2*seq])\n",
    "generated_data = generator(noise, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0IKnaCtg6WE"
   },
   "source": [
    "## The Discriminator\n",
    "\n",
    "A simple CNN binary classifier. Is it real of fake?\n",
    "The discriminator will classify generated data as real or fake. \n",
    "**Positive = real** data and **negative = fake**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dw2tPLmk2pEP"
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    #start with your 2xseq THEN DOWN SAMPLE BEFORE MAKING THE FINAL CALL.\n",
    "    model.add(layers.Conv2D(64, (2, 100), strides=(1, 2), padding='same',\n",
    "                                     input_shape=[2, seq, 1],dtype='float32'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (2, 50), strides=(1, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (2, 25), strides=(1, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (2, 5), strides=(1, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(256, (2, 5), strides=(1, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(512, (2, 5), strides=(1, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDkA05NE6QMs"
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()\n",
    "decision = discriminator(generated_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## Define the losses\n",
    "\n",
    "We need one function for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "psQfmXxYKU3X"
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKY_iPSPNWoj"
   },
   "source": [
    "## Discriminator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    # if the discriminator (or gen bad) did well, all real are 1, loss =0\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    \n",
    "    # If discriminator did well or gen bad, will be zero\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jd-3GCUEiKtv"
   },
   "source": [
    "## Generator loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MgIc7i0th_Iu"
   },
   "source": [
    "## Define the optimzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iWCn_PVdEJZ7"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(3e-6)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(3e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWtinsGDPJlV"
   },
   "source": [
    "## Save updates\n",
    "In case of interrupts, should automatically restart. Code care of Google TensorFlow tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CA1w-7s2POEy"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = 'training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Define the training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NS2GWywBbAWo"
   },
   "outputs": [],
   "source": [
    "noise_dim =2*seq\n",
    "num_examples_to_generate = 3\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "noise = tf.random.normal([BATCH_SIZE, noise_dim],stddev=masternoise,seed=7)\n",
    "\n",
    "def compute_pairwise_distances(x, y):\n",
    "  \"\"\"Computes the squared pairwise Euclidean distances between x and y.\n",
    "  Args:\n",
    "    x: a tensor of shape [num_x_samples, num_features]\n",
    "    y: a tensor of shape [num_y_samples, num_features]\n",
    "  Returns:\n",
    "    a distance matrix of dimensions [num_x_samples, num_y_samples].\n",
    "  Raises:\n",
    "    ValueError: if the inputs do no matched the specified dimensions.\n",
    "  \"\"\"\n",
    "\n",
    "  if not len(x.get_shape()) == len(y.get_shape()) == 2:\n",
    "    raise ValueError('Both inputs should be matrices.')\n",
    "\n",
    "  if x.get_shape().as_list()[1] != y.get_shape().as_list()[1]:\n",
    "    raise ValueError('The number of features should be the same.')\n",
    "\n",
    "  norm = lambda x: tf.reduce_sum(tf.square(x), 1)\n",
    "\n",
    "  # By making the `inner' dimensions of the two matrices equal to 1 using\n",
    "  # broadcasting then we are essentially substracting every pair of rows\n",
    "  # of x and y.\n",
    "  # x will be num_samples x num_features x 1,\n",
    "  # and y will be 1 x num_features x num_samples (after broadcasting).\n",
    "  # After the substraction we will get a\n",
    "  # num_x_samples x num_features x num_y_samples matrix.\n",
    "  # The resulting dist will be of shape num_y_samples x num_x_samples.\n",
    "  # and thus we need to transpose it again.\n",
    "  return tf.transpose(norm(tf.expand_dims(x, 2) - tf.transpose(y)))\n",
    "\n",
    "\n",
    "def gaussian_kernel_matrix(x, y, sigmas):\n",
    "  r\"\"\"Computes a Guassian Radial Basis Kernel between the samples of x and y.\n",
    "  We create a sum of multiple gaussian kernels each having a width sigma_i.\n",
    "  Args:\n",
    "    x: a tensor of shape [num_samples, num_features]\n",
    "    y: a tensor of shape [num_samples, num_features]\n",
    "    sigmas: a tensor of floats which denote the widths of each of the\n",
    "      gaussians in the kernel.\n",
    "  Returns:\n",
    "    A tensor of shape [num_samples{x}, num_samples{y}] with the RBF kernel.\n",
    "  \"\"\"\n",
    "\n",
    "  x=tf.cast(x,dtype=tf.float32)\n",
    "  y=tf.cast(y,dtype=tf.float32)\n",
    "  beta = 1. / (2. * (tf.expand_dims(sigmas, 1)))\n",
    "\n",
    "  dist = compute_pairwise_distances(x, y)\n",
    "\n",
    "  s = tf.matmul(beta, tf.reshape(dist, (1, -1)))\n",
    "\n",
    "  return tf.reshape(tf.reduce_sum(tf.exp(-s), 0), tf.shape(dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import utils\n",
    "\n",
    "sigmas = [\n",
    "      1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 5, 10, 15, 20, 25, 30, 35, 100,\n",
    "      1e3, 1e4, 1e5, 1e6\n",
    "  ]\n",
    "gaussian_kernel = partial(\n",
    "    gaussian_kernel_matrix, sigmas=tf.constant(sigmas))\n",
    "\n",
    "\n",
    "def maximum_mean_discrepancy(x, y,kernel=gaussian_kernel):\n",
    "\n",
    "    r\"\"\"Computes the Maximum Mean Discrepancy (MMD) of two samples: x and y.\n",
    "\n",
    "    Maximum Mean Discrepancy (MMD) is a distance-measure between the samples of\n",
    "    the distributions of x and y. Here we use the kernel two sample estimate\n",
    "    using the empirical mean of the two distributions.\n",
    "\n",
    "    MMD^2(P, Q) = || \\E{\\phi(x)} - \\E{\\phi(y)} ||^2\n",
    "    = \\E{ K(x, x) } + \\E{ K(y, y) } - 2 \\E{ K(x, y) },\n",
    "\n",
    "    where K = <\\phi(x), \\phi(y)>,\n",
    "    is the desired kernel function, in this case a radial basis kernel.\n",
    "\n",
    "    Args:\n",
    "    x: a tensor of shape [num_samples, num_features]\n",
    "    y: a tensor of shape [num_samples, num_features]\n",
    "    kernel: a function which computes the kernel in MMD. Defaults to the\n",
    "    GaussianKernelMatrix.\n",
    "\n",
    "    Returns:\n",
    "    a scalar denoting the squared maximum mean discrepancy loss.\n",
    "    \"\"\"\n",
    "    #with tf.name_scope('MaximumMeanDiscrepancy'):\n",
    "        # \\E{ K(x, x) } + \\E{ K(y, y) } - 2 \\E{ K(x, y) }\n",
    "    cost = tf.reduce_mean(kernel(x, x))\n",
    "    cost += tf.reduce_mean(kernel(y, y))\n",
    "    cost -= 2 * tf.reduce_mean(kernel(x, y))\n",
    "\n",
    "    # We do not allow the loss to become negative.\n",
    "    cost = tf.where(cost > 0, cost, 0, name='value')\n",
    "    return cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3t5ibNo05jCB"
   },
   "outputs": [],
   "source": [
    "# Since this is a compiled function it is difficult to modify on the fly.\n",
    "@tf.function\n",
    "def train_step(images,train_disc):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim],stddev=masternoise)\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "      #could Dr the data here to binarise the idl row\n",
    "      # and even do something else..\n",
    "      #how many real images does the descriminator think are correct?\n",
    "      real_output = discriminator(images, training=train_disc)\n",
    "      # So if the DISCRIMINATOR is perfect, it will return 1(s).\n",
    "   \n",
    "      # How many fake images does the discriminator detect?\n",
    "      fake_output = discriminator(generated_images, training=train_disc)\n",
    "      # So if DISCRIMINATOR is perfect, this would be all -1s\n",
    "      # if DISCRIMINATOR is poor this will be all 1s\n",
    "      # ...but of course if GENERATOR is perfect it will also be 1s\n",
    "      # So I am thinking now; if we rip out a generated_data and find that that the idealisation says just one number\n",
    "      # when rounded. probably that was no good so should multiply the losses by some factor\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      # if the data were perfect (or discriminator awful) this will be tiny.\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "      # if gen bad or discrim great, this is going to be zero\n",
    "            \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return disc_loss, gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newLR(LR, midlr, LRbase, LRmax):\n",
    "    a=1\n",
    "    '''do nothing just testing'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs,startep=0):\n",
    "  import random\n",
    "  global looser, glooser\n",
    "  loss=0\n",
    "  gloss=0\n",
    "  train_disc=True\n",
    "  noise=False\n",
    "  flip = False\n",
    "  mu=0\n",
    "  dlr=3e-7\n",
    "  glr=1e-5\n",
    "\n",
    "  sigma=0.0001\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "        if noise==True:\n",
    "          #Add noise Clearly should use a tf.random not python!\n",
    "          for image in image_batch:\n",
    "            b=tf.dtypes.cast(tf.random.normal([seq], mean=mu, stddev=sigma),tf.float64)\n",
    "            a=tf.dtypes.cast(image[0, :, 0],tf.float64)\n",
    "            a=tf.math.add(a,b)\n",
    "            b=tf.dtypes.cast(image[1, :, 0],tf.float64)\n",
    "            image=tf.stack([a,b])\n",
    "            #Shape has changed from (2,seq,1) to (2,seq)\n",
    "        if flip == True:\n",
    "            for image in image_batch:\n",
    "              if random.randint(1, 2)==1:\n",
    "                image = tf.image.flip_left_right(image)\n",
    "        loss, genl= train_step(image_batch,train_disc)\n",
    "        if genl>10*loss:\n",
    "            #deactivated this now\n",
    "            train_disc=True\n",
    "        else:\n",
    "            train_disc=True\n",
    "\n",
    "    glooser.append(genl.numpy().item())   \n",
    "    looser.append(loss.numpy().item())\n",
    "    \n",
    "    # Produce images for a GIF as we go\n",
    "    # Save the model every x epochs\n",
    "    interceptor=50\n",
    "    if (epoch + 1) % interceptor == 0:\n",
    "      glr_up=False\n",
    "      if sum(looser[-interceptor:])/len(looser[-interceptor:])<0.2:\n",
    "        dlr= tf.keras.backend.get_value(discriminator_optimizer.learning_rate)*.75\n",
    "        tf.keras.backend.set_value(discriminator_optimizer.learning_rate,dlr)\n",
    "      elif sum(looser[-interceptor:])/len(looser[-interceptor:])>1:\n",
    "        dlr= tf.keras.backend.get_value(discriminator_optimizer.learning_rate)*1.5\n",
    "        tf.keras.backend.set_value(discriminator_optimizer.learning_rate,dlr) \n",
    "      if sum(glooser[-interceptor:])/len(glooser[-interceptor:])<0.2:\n",
    "        glr= tf.keras.backend.get_value(generator_optimizer.learning_rate)*.99\n",
    "        tf.keras.backend.set_value(generator_optimizer.learning_rate,glr)\n",
    "      elif sum(glooser[-interceptor:])/len(glooser[-interceptor:])>3:\n",
    "        glr_up=True\n",
    "        glr= tf.keras.backend.get_value(generator_optimizer.learning_rate)*1.01\n",
    "        if glr >1e-2:\n",
    "            glr=1e-2\n",
    "        tf.keras.backend.set_value(generator_optimizer.learning_rate,glr)        \n",
    "    \n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "      # Generate after the final epoch\n",
    "      display.clear_output(wait=True)\n",
    "      generate_and_save_images(generator,\n",
    "                           int(epoch+startep),\n",
    "                           seed)\n",
    "      print(\"Train discriminator was \",train_disc)\n",
    "      with open('somefile.txt', 'a') as the_file:\n",
    "        the_file.write('Glr_up {}, Time for epoch {} is {:.0f} sec, DLR={:.2e}, GLR={:.2e}, loss {:.4f} gloss {:.2f}\\n'.format(glr_up,\n",
    "           epoch + startep,time.time()-start,dlr, glr, sum(looser[-interceptor:])/len(looser[-interceptor:]),\n",
    "            sum(glooser[-interceptor:])/len(glooser[-interceptor:])))\n",
    "      if len (looser)>interceptor:\n",
    "        print ('Glr_up {}, Time for epoch {} is {:.0f} sec, DLR={:.2e}, GLR={:.2e}, loss {:.4f} gloss {:.2f}'.format(glr_up,\n",
    "           epoch + startep,time.time()-start,dlr, glr, sum(looser[-interceptor:])/len(looser[-interceptor:]),\n",
    "            sum(glooser[-interceptor:])/len(glooser[-interceptor:])))\n",
    "      else:\n",
    "        print ('Time for epoch {} is {:.0f} sec, loss {:.4f} gloss {:.2f}'.format(epoch + startep,\n",
    "          time.time()-start,sum(looser)/len(looser),sum(glooser)/len(glooser)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aFF7Hk3XdeW"
   },
   "source": [
    "**Generate and save images**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.  \n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  # want to pick a different set each time I do\n",
    "  global xout, yout, ep, SD\n",
    "  num_examples_to_generate=4 #caution this is a REDEFINITION\n",
    "  crop=200\n",
    "  seconds = seconds = int(time.time()-start)\n",
    "  tf.random.set_seed(seconds)\n",
    "  seed = tf.random.normal([num_examples_to_generate, noise_dim], stddev=1)\n",
    "  offset = False\n",
    "  test_input=seed\n",
    "  predictions = model(test_input, training=False)\n",
    "  x=tf.convert_to_tensor(mtrain_images[0:num_examples_to_generate,:,:,0].reshape(num_examples_to_generate,2*1280))\n",
    "  y=tf.reshape(predictions[0:num_examples_to_generate,:,:,0],shape=(num_examples_to_generate,2*1280))\n",
    "\n",
    "  #tf.convert_to_tensor(data_np, np.float32)\n",
    "  #print(maximum_mean_discrepancy(x,y))\n",
    "  #ms.append(maximum_mean_discrepancy(x,y))\n",
    "  \n",
    "  #x = np.array([[1,1], [2,2], [3,3], [4,4], [5,5]])\n",
    "  #y = np.array([[2,2], [3,3], [4,4]])\n",
    "  #fastdist, path = fastdtw(x, y, dist=euclidean)\n",
    "  #print(\"Dynamic Time Warping metric:\")\n",
    "  #print(fastdist)\n",
    "  #dtw.append(fastdist)\n",
    "  fig = plt.figure(figsize=(12,4))\n",
    "  seqfig=seq-(crop*2)\n",
    "  xax = range(seqfig*predictions.shape[0])\n",
    "  data=[]\n",
    "  labs=[]\n",
    "  for i in range(predictions.shape[0]):\n",
    "      newdata=predictions[i, 0, crop:seq-crop, 0] #do some cropping to clear glitch\n",
    "      newlabs=predictions[i, 1, crop:seq-crop, 0]\n",
    "      newlabs=np.asarray(newlabs)\n",
    "      newlabs=np.round(1.5+newlabs/2)\n",
    "      newlabs=list(newlabs)       \n",
    "      if offset:\n",
    "          # Offset to match-up the zero levels\n",
    "          zeros=[i for i, e in enumerate(newlabs) if e == 2]\n",
    "          if len(zeros)>0:\n",
    "              offers=np.asarray(newdata)\n",
    "              offset=np.mean(offers[zeros])\n",
    "              # offset=sum(data[zeros])/len(zeros)\n",
    "              newdata=[e-offset for e in newdata]  \n",
    "      data.extend(newdata)\n",
    "      labs.extend(newlabs)\n",
    "      \n",
    "      #plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "  if (epoch-1)%20 == 0:\n",
    "    xout.extend([tens.numpy() for tens in data])\n",
    "    yout.extend(labs)\n",
    "    ep.extend(len(data) * [epoch-1])\n",
    "  realdat=[]\n",
    "  reallab=[]\n",
    "  for im in range(num_examples_to_generate):\n",
    "     limage=rd.choice(range(len(mtrain_images)))\n",
    "     realdat.extend(mtrain_images[limage,0, crop:seq-crop, 0])\n",
    "     reallab.extend(mtrain_images[limage,1, crop:seq-crop, 0])\n",
    "  reallab=[1.5+val/2 for val in reallab]\n",
    "  #realdat=[2*val for val in realdat]\n",
    "  if CARTOON==False:  \n",
    "      plt.subplot(4,1,1)\n",
    "      plt.plot(xax,realdat)\n",
    "      plt.plot(xax,reallab)  \n",
    "      plt.subplot(4,1,2)\n",
    "      plt.plot(xax,data)\n",
    "      plt.plot(xax,np.round(labs))\n",
    "      plt.hlines(0, 0, seqfig*predictions.shape[0],linestyles='dashed')\n",
    "      vs=[seqfig,2*seqfig,3*seqfig]\n",
    "      plt.vlines(vs,-0.5,2,linestyles='dashed')  \n",
    "      #plt.ylim([-1,2.1])\n",
    "      plt.axis('on')\n",
    "      plt.subplot(4,1,3)\n",
    "      plt.plot(range(len(looser)),looser,'o')\n",
    "      plt.plot(range(len(glooser)), np.asarray(glooser)/4, 'o')\n",
    "      #print(\"prediction SD = {}\".format(np.std(np.asarray([el for el in data if el < 0]))))\n",
    "      sdtemp=[el for el in data if el > 0]\n",
    "      sdtemp=np.asarray(sdtemp)\n",
    "      sdtemp=np.std(sdtemp).item() \n",
    "      SD.append(sdtemp)\n",
    "      plt.subplot(4,1,4)\n",
    "      plt.plot(range(len(SD)),SD)\n",
    "  else:\n",
    "      plt.subplot(2,1,1)\n",
    "      plt.plot(xax,realdat)\n",
    "      plt.plot(xax,reallab)  \n",
    "      plt.hlines(0, 0, seqfig*predictions.shape[0],linestyles='dashed')\n",
    "      plt.subplot(2,1,2)\n",
    "      plt.plot(xax,data)\n",
    "      plt.plot(xax,np.round(labs))\n",
    "      plt.hlines(0, 0, seqfig*predictions.shape[0],linestyles='dashed')\n",
    "      plt.axis('on')\n",
    "  \n",
    "  plt.savefig(\"images/images_at_epoch_{:06d}.png\".format(epoch),jpg=120)\n",
    "  plt.show()\n",
    "  \n",
    "  if epoch==EPOCHS-1:\n",
    "    np.savetxt('mmd.csv', ms, delimiter=',')\n",
    "    np.savetxt('dtw.csv', dtw, delimiter=',')\n",
    "    np.savetxt('mmd.csv', ms, delimiter=',')\n",
    "    out=np.vstack([xout,yout,ep])\n",
    "    print(\"outshape\",out)\n",
    "    np.savetxt('synth.csv', out.T, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dZrd4CdjR-Fp"
   },
   "source": [
    "## Train!!\n",
    "Sometimes necessary to tinker with the training rates! Losses should balance, but generator loss always much greater when running well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ly3UN0SLLY2l",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "looser=[]\n",
    "glooser=[]\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "tf.keras.backend.set_value(discriminator_optimizer.learning_rate,3e-7) \n",
    "tf.keras.backend.set_value(generator_optimizer.learning_rate,3e-6) \n",
    "\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4M_vIbUi7c0"
   },
   "source": [
    "## Create a GIF/MOVIES for demonstration purposes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NywiH3nL8guF"
   },
   "source": [
    "###### Use `imageio` to create an animated gif using the images saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGKQgENQ8lEI"
   },
   "outputs": [],
   "source": [
    "'''anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I',fps=2) as writer:\n",
    "  filenames = glob.glob('images/image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  last = -1\n",
    "  for i,filename in enumerate(filenames):\n",
    "    frame = 2*(i**0.5)\n",
    "    if round(frame) > round(last):\n",
    "      last = frame\n",
    "    else:\n",
    "      continue\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "import IPython\n",
    "if IPython.version_info > (6,2,0,''):\n",
    "  display.Image(filename=anim_file)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.mp4'\n",
    "with imageio.get_writer(anim_file,fps=20) as writer:\n",
    "  filenames = glob.glob('images/image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "anim_file = 'dcgan.gif'\n",
    "with imageio.get_writer(anim_file,fps=4) as writer:\n",
    "  filenames = glob.glob('images/image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  decimate=2\n",
    "  for count,filename in enumerate(filenames):\n",
    "    if count%decimate==0:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k6qC-SbjK0yW"
   },
   "source": [
    "## Collate data for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('loss.csv', looser, delimiter=',')\n",
    "np.savetxt('gloss.csv', glooser, delimiter=',')\n",
    "np.savetxt('mmd.csv', ms, delimiter=',')\n",
    "np.savetxt('dtw.csv', dtw, delimiter=',')\n",
    "np.savetxt('mmd.csv', ms, delimiter=',')\n",
    "out=np.vstack([xout,yout,ep])\n",
    "np.savetxt('synth.csv', out.T, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.vstack([xout,yout,ep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('images/image*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"images/images_at_epoch_{:06d}.png\".format(15900+300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from PIL import Image, ImageDraw, ImageSequence, ImageFont\n",
    "import io\n",
    "anim_file = 'dcganlabout.mp4'\n",
    "im = Image.open('dcganlab.gif')\n",
    "transparent = (255,0,0,0)\n",
    "bkg = Image.new('RGBA',(200,100),transparent)\n",
    "# A list of the frames to be output\n",
    "font = ImageFont.truetype(\"calibri.ttf\", 25)\n",
    "frames = []\n",
    "i=0\n",
    "\n",
    "for frame in ImageSequence.Iterator(im):\n",
    "    i+=1\n",
    "    frame = frame.convert('RGBA')\n",
    "    # Draw the text on the frame\n",
    "    im.paste(bkg,(110,40))\n",
    "    d = ImageDraw.Draw(frame)\n",
    "    d.text((110,40), \"Real idealisation\", fill=(0,0,0),font=font)\n",
    "    d.text((110,100), \"Real raw data\", fill=(0,0,0),font=font)\n",
    "    d.text((110,160), \"GAN idealisation\", fill=(0,0,0),font=font)\n",
    "    d.text((110,220), \"GAN raw data\", fill=(0,0,0),font=font)    \n",
    "\n",
    "\n",
    "    del d\n",
    "\n",
    "    b = io.BytesIO()\n",
    "    frame.save(b, format=\"GIF\")\n",
    "    frame = Image.open(b)\n",
    "\n",
    "    # Then append the single frame image to a list of frames\n",
    "    frames.append(frame)\n",
    "# Save the frames as a new image\n",
    "frames[0].save('GANTEXT.gif', save_all=True, append_images=frames[1:])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpy\n",
    "ff = ffmpy.FFmpeg(\n",
    "inputs={'GANTEXT.gif': None},\n",
    "    outputs={'GANTEXT.mp4': None})\n",
    "ff.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mp\n",
    "\n",
    "clip = mp.VideoFileClip(\"GANTEXT.gif\")\n",
    "clip.write_videofile(\"GANTEXT.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
