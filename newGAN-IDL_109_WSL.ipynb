{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd8be84-d9b7-4c77-8cc3-8cce6c6f7a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 15:29:54.684916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-11 15:29:54.721919: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-11 15:29:54.735423: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-11 15:29:54.762903: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-11 15:29:56.188496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "tf.config.run_functions_eagerly(True)\n",
    "size=200\n",
    "filterSilence=True\n",
    "#Run with or without tensorflow_probability\n",
    "TFTSIM = False\n",
    "\"\"\"\n",
    "TODO:\n",
    "Could select only real data with events. Done\n",
    "Import my save.txt routine to optionally write output.\n",
    "Rename the To TC with rate constants K13 K12 K23 etc.\n",
    "Would another state help? I don't think so... 4 states should capture realistic bursting.\n",
    "Could draw the transition matrix as a graph too? Too fancy. User can do this.\n",
    "Noise could be more authentic still. \n",
    "a. Some slow-wave. freq, phase, amp.\n",
    "b. Some open channel noise. tf.boolean_mask ...if open add white noise? could be slow.\n",
    "c. replace noise with Sam noise? was it all TF?\n",
    "d. SPIKE GENERATION SHOULD BE CHANGED, max should be x not abs(x) and number should be abs(x) not be abs(x) +1. X\n",
    "e. pink noise should also be tf.function decorated!! X\n",
    "f. WE ARE USING PROBABILITIES NOT RATES SO NEED TO ADAPT OR CORRECT BEFORE WRITING TO FILE!\n",
    "\n",
    "F:: concerned about whether I have consistent use of dt.  In some places it seems with have an array that is T long. In other places this is called \n",
    "size and sometimes steps are 1 long and others they are df long @@\n",
    "OK ...looks like the only time dt is used is when plotting simulated output.\n",
    "\n",
    "CHANNEL IS LANE 0\n",
    "RAW IS LANE 1\n",
    "\n",
    "RUNNING ON WSL2 Moonbase computer with env101. TF version 2.17 I believe.\n",
    "\"\"\"\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # '0' = all logs, '1' = filter out INFO logs, '2' = filter out WARNING logs, '3' = filter out ERROR logs\n",
    "\n",
    "# Suppress other warnings\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# Additional suppression for TensorFlow 2.x\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\"\"\"claude suggested this can improve performance\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\"\"\"\n",
    "\n",
    "# List all physical GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "print(f\"Number of GPUs detected: {len(gpus)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42709ba-2731-4e53-94ef-08b800ec8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1  # Number of channels\n",
    "dt = tf.constant(0.1, dtype=tf.float32)\n",
    "T = tf.constant(size, dtype=tf.int32)  # In sample points :-)\n",
    "#Must be a multiple of 2!!!\n",
    "#Size of channel (relative to the channels so one channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871588c-0a89-4101-9e00-6f44be354925",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_compile=True)\n",
    "def tf_relaxation(binary_sequence, half_life=4.0, relaxation_amount=0.2):\n",
    "    \"\"\"\n",
    "    Apply exponential relaxation to a 1D binary sequence using vectorized TensorFlow operations.\n",
    "    \n",
    "    Args:\n",
    "    binary_sequence: tf.Tensor, shape [time_steps], sequence of 0s and 1s\n",
    "    half_life: float, the half-life of the exponential decay\n",
    "    relaxation_amount: float, the amount of relaxation (positive or negative)\n",
    "    Kind of works backward to what you might think.  Where amp i1 1 and relaxation 0f 0.5 is too 0.5 therefore\n",
    "    relaxation -0.5 is 0 to 1 but it then slowly rising to 1.5\n",
    "    Returns:\n",
    "    tf.Tensor, shape [time_steps]\n",
    "    \"\"\"\n",
    "    # Convert input to float32\n",
    "    binary_sequence = tf.cast(binary_sequence, tf.float32)\n",
    "    \n",
    "    # Calculate decay rate\n",
    "    decay_rate = tf.math.log(2.0) / half_life\n",
    "    \n",
    "    # Find the indices where steps occur\n",
    "    steps = tf.not_equal(binary_sequence[1:] - binary_sequence[:-1], 0)\n",
    "    step_indices = tf.where(steps)[:, 0]\n",
    "    \n",
    "    # Calculate the time since each step\n",
    "    time_steps = tf.range(tf.shape(binary_sequence)[0], dtype=tf.float32)\n",
    "    time_since_step = time_steps[:, tf.newaxis] - tf.cast(step_indices, tf.float32)\n",
    "    \n",
    "    # Calculate the exponential decay for each step\n",
    "    decay = tf.exp(-decay_rate * tf.maximum(time_since_step, 0.0))\n",
    "    \n",
    "    # Calculate the relaxation effect\n",
    "    step_values = tf.gather(binary_sequence, step_indices + 1) - tf.gather(binary_sequence, step_indices)\n",
    "    relaxation_effect = relaxation_amount * step_values * decay\n",
    "    \n",
    "    # Sum the effects of all steps\n",
    "    total_relaxation = tf.reduce_sum(relaxation_effect, axis=1)\n",
    "    \n",
    "    # Add the relaxation to the original sequence\n",
    "    relaxed_sequence = binary_sequence + total_relaxation\n",
    "    \n",
    "    return relaxed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74e988-df26-42e4-b4f9-8744dedd8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test sequence\n",
    "test_sequence = tf.constant([0, 0, 1, 1, 1, 1, 0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,0,0,0], dtype=tf.float32)\n",
    "\n",
    "# Apply the relaxation\n",
    "relaxed_sequence = tf_relaxation(test_sequence, relaxation_amount=-0.5)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(test_sequence.numpy(), label='Original')\n",
    "plt.plot(relaxed_sequence.numpy(), label='Relaxed')\n",
    "plt.legend()\n",
    "plt.title('Exponential Relaxation')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5426666-b59b-49f1-98bf-55ddfd3c7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "And entirely replaced function, by Claude now that doesn't use tf.probability\n",
    "it seems. Is it really the same?\n",
    "This needs to be checked. Certainly much faster this way. About 5x faster... it is getting 10% of the GPU cuda use now. was barely visible before.\n",
    "\"\"\"\n",
    "\n",
    "@tf.function(experimental_compile=True)\n",
    "def normalize_row(row):\n",
    "    # Find non-zero elements\n",
    "    non_zero_mask = tf.not_equal(row, 0.0)\n",
    "    non_zero_sum = tf.reduce_sum(tf.boolean_mask(row, non_zero_mask))\n",
    "    \n",
    "    # Normalize only non-zero elements\n",
    "    normalized_row = tf.where(\n",
    "        non_zero_mask,\n",
    "        row / non_zero_sum,\n",
    "        row\n",
    "    )      \n",
    "    return normalized_row\n",
    "\n",
    "if not TFTSIM:\n",
    "    @tf.function(experimental_compile=True)\n",
    "    def sim_channel(params):\n",
    "        #This is about twice the speed!\n",
    "        pc12, pc21, relaxation, Fnoise, scale, offset, relaxT, pco1, poc2, po12, po21 = params\n",
    "        zero = tf.constant(0.0, dtype=tf.float32)\n",
    "    \n",
    "        # Markov chain simulation\n",
    "        \n",
    "        row1 = tf.stack([1-pc12, pc12, zero, zero])\n",
    "        row2 = tf.stack([pc21, 1-pc21-pco1, pco1, zero])\n",
    "        row3 = tf.stack([zero, poc2, 1-poc2-po12, po12])\n",
    "        row4 = tf.stack([zero, zero, po21, 1-po21])\n",
    "        \n",
    "        unnormalized_matrix = tf.stack([row1, row2, row3, row4])\n",
    "\n",
    "        # Normalize each row while preserving zero probabilities\n",
    "        transition_matrix = tf.map_fn(normalize_row, unnormalized_matrix)\n",
    "    \n",
    "        initial_distribution = tfp.distributions.Categorical(probs=[0.3, 0.3, 0.2, 0.2])\n",
    "     \n",
    "        # Initial state distribution\n",
    "        initial_probs = tf.constant([0.3, 0.3, 0.2, 0.2])\n",
    "        \n",
    "        # Manual Markov chain simulation\n",
    "        tf.random.set_seed(int(time.time() * 1000) % (2**31 - 1))\n",
    "        def body(i, state, channels):\n",
    "            next_state_probs = tf.gather(transition_matrix, state)\n",
    "            next_state = tf.random.categorical(tf.math.log([next_state_probs]), num_samples=1)[0, 0]\n",
    "            channels = channels.write(i, tf.cast(tf.greater_equal(next_state, 2), tf.float32))\n",
    "            return i+1, next_state, channels\n",
    "    \n",
    "        initial_state = tf.random.categorical(tf.math.log([initial_probs]), num_samples=1)[0, 0]\n",
    "        channels = tf.TensorArray(tf.float32, size=T)\n",
    "        _, _, channels = tf.while_loop(\n",
    "            lambda i, *_: i < T,\n",
    "            body,\n",
    "            (0, initial_state, channels)\n",
    "        )\n",
    "        \n",
    "        channels = channels.stack()\n",
    "        channels = tf.squeeze(channels)\n",
    "    \n",
    "        # Generate pink noise\n",
    "        white_noise = tf.random.normal(shape=[T])\n",
    "        fft_len = T // 2 + 1\n",
    "        f = tf.range(1, fft_len, dtype=tf.float32)\n",
    "        spectrum = 1.0 / tf.sqrt(f)\n",
    "        spectrum = tf.concat([tf.constant([1.0]), spectrum], axis=0)\n",
    "        white_noise_fft = tf.signal.rfft(white_noise)\n",
    "        pink_noise_fft = white_noise_fft * tf.cast(spectrum, tf.complex64)\n",
    "        pink_noise = tf.signal.irfft(pink_noise_fft)\n",
    "        pink_noise -= tf.reduce_mean(pink_noise)\n",
    "        pink_noise = pink_noise / tf.math.reduce_std(pink_noise)\n",
    "        noise = pink_noise * Fnoise\n",
    "    \n",
    "        # Add relaxation\n",
    "        modified_raw_column = (channels * scale) + offset\n",
    "        modified_raw_column = tf_relaxation(modified_raw_column, half_life=relaxT, relaxation_amount=relaxation)\n",
    "        \n",
    "        modified_raw_column += noise\n",
    "    \n",
    "        # Combine channels and modified raw column\n",
    "        image = tf.stack([channels, modified_raw_column], axis=1)\n",
    "        \n",
    "        # Final safeguard against NaN values\n",
    "        image = tf.where(tf.math.is_nan(image), tf.zeros_like(image), image)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe723a0-f033-435b-af49-91566dc458af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TFTSIM:\n",
    "    @tf.function(experimental_compile=True)\n",
    "    def sim_channel(params, num_steps=T):\n",
    "        pc12, pc21, relaxation, Fnoise, scale, offset, relaxT, pco1, poc2, po12, po21 = params\n",
    "        zero = tf.constant(0.0, dtype=tf.float32)\n",
    "        \n",
    "        unnormalized_matrix = tf.stack([\n",
    "             #s0     #s1  #s2   #s3\n",
    "            [1-pc12, pc12, zero,   zero],          # state 0 CLOSED\n",
    "            [pc21, 1-pc21-pco1, pco1, zero],   # state 1 CLOSED\n",
    "            [zero, poc2, 1-poc2-po12, po12],   # state 2 OPEN\n",
    "            [zero,    zero,   po21, 1-po21]        # state 3 OPEN\n",
    "        ])\n",
    "        \n",
    "        # Normalize each row while preserving zero probabilities\n",
    "        transition_matrix = tf.map_fn(normalize_row, unnormalized_matrix)\n",
    "    \n",
    "        initial_distribution = tfp.distributions.Categorical(probs=[0.3, 0.3, 0.2, 0.2])\n",
    "        \n",
    "        markov_chain = tfp.distributions.MarkovChain(\n",
    "            initial_state_prior=initial_distribution,\n",
    "            transition_fn=lambda _, state: tfp.distributions.Categorical(probs=tf.gather(transition_matrix, state)),\n",
    "            num_steps=num_steps\n",
    "        )\n",
    "        states = markov_chain.sample()\n",
    "        #Assuming we have one channel so emission is...\n",
    "        emissions = tf.where(tf.less(states, 2), tf.zeros_like(states), tf.ones_like(states))\n",
    "    \n",
    "        channels = tf.cast(emissions, dtype=tf.float32)\n",
    "        # Generate pink noise\n",
    "        white_noise = tf.random.normal(shape=[T])\n",
    "        fft_len = T // 2 + 1\n",
    "        f = tf.range(1, fft_len, dtype=tf.float32)\n",
    "        spectrum = 1.0 / tf.sqrt(f)\n",
    "        spectrum = tf.concat([tf.constant([1.0]), spectrum], axis=0)\n",
    "        white_noise_fft = tf.signal.rfft(white_noise)\n",
    "        pink_noise_fft = white_noise_fft * tf.cast(spectrum, tf.complex64)\n",
    "        pink_noise = tf.signal.irfft(pink_noise_fft)\n",
    "        pink_noise -= tf.reduce_mean(pink_noise)\n",
    "        pink_noise = pink_noise / tf.math.reduce_std(pink_noise)\n",
    "        noise = pink_noise * Fnoise\n",
    "    \n",
    "        # Add relaxation\n",
    "        modified_raw_column = (channels * scale) + offset\n",
    "        modified_raw_column = tf_relaxation(modified_raw_column, half_life=relaxT, relaxation_amount=relaxation)\n",
    "        \n",
    "        modified_raw_column += noise\n",
    "    \n",
    "        # Combine channels and modified raw column\n",
    "        image = tf.stack([channels, modified_raw_column], axis=1)\n",
    "        \n",
    "        # Final safeguard against NaN values\n",
    "        image = tf.where(tf.math.is_nan(image), tf.zeros_like(image), image)\n",
    "        \n",
    "        return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322af487-6e57-4de2-8d21-0d99711234c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the exponential distribution\n",
    "num_samples = 5\n",
    "\"\"\"pc12, pc21, relaxation, Fnoise, scale, offset, relaxT, pco1, poc2, po12, po21\"\"\"\n",
    "pc12 = tf.constant(.3, dtype=tf.float32)  # Decreased from 0.1\n",
    "pc21 = tf.constant(.8, dtype=tf.float32)  # Decreased from 0.1\n",
    "pco1 = tf.constant(0.3, dtype=tf.float32)  # Decreased from 0.01\n",
    "poc2 = tf.constant(1e-1, dtype=tf.float32)  # Decreased from 0.0001\n",
    "po12 = tf.constant(0.8, dtype=tf.float32)  # Decreased from 0.01\n",
    "po21 = tf.constant(1e-1, dtype=tf.float32)\n",
    "\n",
    "\n",
    "relaxation = tf.constant(0.5, dtype=tf.float32)\n",
    "Fnoise = tf.constant(.04, dtype=tf.float32)\n",
    "SCALE = tf.constant(.6, dtype=tf.float32)\n",
    "#And an offset\n",
    "OFFSET = tf.constant(-0.4, dtype=tf.float32)\n",
    "relaxT=25\n",
    "# nE = tf.constant(200, dtype=tf.int32) #number of events\n",
    "\n",
    "\n",
    "# Generate training data\n",
    "training_data = []\n",
    "lens=[]\n",
    "\"\"\"Actually replace \"Anoise\" with relaxation later\"\"\"\n",
    "for sample in tqdm(range(num_samples)):   \n",
    "    params = tf.stack([pc12, pc21, relaxation, Fnoise, SCALE, OFFSET, relaxT, pco1, poc2, po12, po21])  # Use tf.stack instead of tf.constant\n",
    "    segment = sim_channel(params)\n",
    "    lens.append(sum(abs(segment)))\n",
    "    training_data.append(segment)\n",
    "print(f\"Average duration was {sum(lens)/len(lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fd338-3df9-4a62-91e1-f5a26648b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(data):\n",
    "    # Create a figure with two subplots (panels)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 6))\n",
    "     # Flatten the axs array for easy iteration\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for i in range(4):\n",
    "        axs[i].plot(data[i])\n",
    "        #axs[i].set_ylim([-200, 200])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotter(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0d52f-d874-4e85-8912-e82b901a8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_compile=True)\n",
    "def prob_to_rate(params, delta_t):\n",
    "    if len(params) == 11:\n",
    "        pc12, pc21, _, _, _, _, _, pco1, poc2, po12, po21 = params\n",
    "    else:\n",
    "        pc12, pc21,pco1, poc2, po12, po21 = params\n",
    "        \n",
    "    zero = tf.constant(0.0, dtype=tf.float32)\n",
    "    \n",
    "    P = tf.stack([\n",
    "        [1-pc12, pc12, zero, zero],\n",
    "        [pc21, 1-pc21-pco1, pco1, zero],\n",
    "        [zero, poc2, 1-poc2-po12, po12],\n",
    "        [zero, zero, po21, 1-po21]\n",
    "    ])\n",
    "    \n",
    "    # Convert probability matrix to rate matrix\n",
    "    Q = (P - tf.eye(tf.shape(P)[0])) / delta_t\n",
    "    \n",
    "    return Q\n",
    "    \n",
    "@tf.function(experimental_compile=True)\n",
    "def append_rates_to_csv(Q, filename='resultsv9.csv'):\n",
    "    param_names = [\"_\", \"kc12\", \"zero\", \"zero\", \n",
    "                   \"kc21\", \"_\", \"kco1\", \"zero\", \n",
    "                   \"zero\", \"koc2\", \"_\", \"ko12\", \n",
    "                   \"zero\", \"zero\", \"ko21\", \"_\"]\n",
    "    \n",
    "    # Flatten the rate matrix\n",
    "    flat_rates = tf.reshape(Q, [-1])\n",
    "    \n",
    "    # Convert to string with comma separation\n",
    "    row_string = tf.strings.join([tf.strings.as_string(rate) for rate in flat_rates], separator=\",\")\n",
    "    \n",
    "    # Add newline character\n",
    "    row_string = tf.strings.join([row_string, tf.constant(\"\\n\")])\n",
    "    \n",
    "    # Check if file exists\n",
    "    file_exists = tf.io.gfile.exists(filename)\n",
    "    \n",
    "    if file_exists:\n",
    "        file_content = tf.io.read_file(filename)\n",
    "        updated_content = tf.strings.join([file_content, row_string])\n",
    "    else:\n",
    "        header = tf.strings.join(param_names, separator=\",\")\n",
    "        header = tf.strings.join([header, tf.constant(\"\\n\")])\n",
    "        updated_content = tf.strings.join([header, row_string])\n",
    "    \n",
    "    # Write the updated content to file\n",
    "    tf.io.write_file(filename, updated_content)\n",
    "\n",
    "\"\"\"# Example usage\n",
    "params = [0.1, 0.2, 0, 0, 0, 0, 0, 0.3, 0.4, 0.5, 0.6]  # Example parameters\n",
    "dt = 0.01  # Example time step\n",
    "\n",
    "Q = prob_to_rate(params, dt)\n",
    "\n",
    "# Print the rate matrix\n",
    "print(\"Rate Matrix Q:\")\n",
    "print(Q.numpy())\n",
    "\n",
    "# Append to CSV\n",
    "append_rates_to_csv(Q)\n",
    "print(\"Rate matrix appended to resultsv9.csv\")\n",
    "\n",
    "# Read and print the contents of the CSV file\n",
    "csv_contents = tf.io.read_file('resultsv9.csv')\n",
    "print(\"\\nContents of resultsv9.csv:\")\n",
    "print(csv_contents.numpy().decode())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea8ea2-3420-4a42-8df7-f9361a049138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create REAL Trainging Data\n",
    "file_path = \"Lina2/4096lina11raw.csv\"\n",
    "df = pd.read_csv(file_path, header=None, names=[\"Raw\", \"Channels\"])\n",
    "df = df[[\"Channels\",\"Raw\"]]\n",
    "# now crop to just one phenotype. There seem multiple in this dataset.\n",
    "df=df[:75000]\n",
    "#df=df[:12000]\n",
    "df = pd.concat([df] * 1, ignore_index=True)\n",
    "noise = np.random.normal(0, 0.01, df[\"Raw\"].shape)\n",
    "df[\"Raw\"] += noise\n",
    "num_rows = (len(df) // size) * size\n",
    "print(num_rows)\n",
    "df = df.iloc[:num_rows]\n",
    "data_array = df.to_numpy()\n",
    "data_tensor = tf.convert_to_tensor(data_array, dtype=tf.float32)\n",
    "training_data = tf.reshape(data_tensor, [-1, size, 2])\n",
    "#Calculate real num_samples!\n",
    "num_samples= tf.shape(training_data)[0]\n",
    "#num_samples = 10 #debug\n",
    "#Only use windows where something happened!\n",
    "filterSilence = False\n",
    "if filterSilence:\n",
    "    first_column = training_data[:, :, 0]\n",
    "    all_same = tf.reduce_all(tf.equal(first_column, first_column[:, 0:1]), axis=1)\n",
    "    \n",
    "    # Filter out batches where all values in the first column are the same\n",
    "    training_data = tf.boolean_mask(training_data, ~all_same)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280dab93-b2e1-4214-b409-ca5f619e2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Raw\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a169f4-e9e4-4000-9a30-ba0653f109a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator model\n",
    "\"\"\"pc12, pc21, relaxation, Fnoise, SCALE, OFFSET, relaxT, pco1, poc2, po12, po21\"\"\"\n",
    "gen_input_len=11\n",
    "\n",
    "def make_generator_model():\n",
    "    \"\"\"batch normalisation is terrible!!\"\"\"\n",
    "    noise_input = tf.keras.layers.Input(shape=(gen_input_len,))\n",
    "    x = tf.keras.layers.Dense(128)(noise_input)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.Dense(256)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    # Output layer without activation\n",
    "    raw_output = tf.keras.layers.Dense(gen_input_len)(x)\n",
    "    \n",
    "    # Apply appropriate activations/scaling to each output\n",
    "    gpc12 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 1e-6, 1e-6, 1.0))(raw_output[:, 0:1])\n",
    "    gpc21 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 1e-6, 1e-6, 1.0))(raw_output[:, 1:2])\n",
    "    grelaxation = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1.0, 1.0))(raw_output[:, 2:3])\n",
    "    gFnoise = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x), 0.0, 1.0))(raw_output[:, 3:4])\n",
    "    gscale = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 0.1, 0.1, 10.0))(raw_output[:, 4:5])\n",
    "    goffset = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1.0, 1.0))(raw_output[:, 5:6])\n",
    "    grelaxT = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) * 100, 1.0, size - 1.0))(raw_output[:, 6:7])\n",
    "    gpco1 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 1e-3, 1e-6, 1.0))(raw_output[:, 7:8])\n",
    "    gpoc2 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 1e-6, 1e-6, 1.0))(raw_output[:, 8:9])\n",
    "    gpo12 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 1e-6, 1e-6, 1.0))(raw_output[:, 9:10])\n",
    "    gpo21 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 1e-6, 1e-6, 1.0))(raw_output[:, 10:11])\n",
    "    \n",
    "    output = tf.keras.layers.Concatenate()([gpc12, gpc21, grelaxation, gFnoise, gscale, goffset, grelaxT, \n",
    "                                            gpco1, gpoc2, gpo12, gpo21])\n",
    "    \n",
    "    return tf.keras.Model(inputs=noise_input, outputs=output)\n",
    "\n",
    "# Define the discriminator model batch, record len, channels = events then noise\n",
    "num_points = T.numpy().item()\n",
    "def make_discriminator_model():\n",
    "    input_shape = (size,2) \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Reshape input to add channel dimension\n",
    "    x = tf.keras.layers.Reshape((size, 2))(inputs)\n",
    "    \n",
    "    # 1D Convolutional layers\n",
    "    x = tf.keras.layers.Conv1D(8, kernel_size=5, strides=2, padding='same', activation='leaky_relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(16, kernel_size=5, strides=2, padding='same', activation='leaky_relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(32, kernel_size=5, strides=2, padding='same', activation='leaky_relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3) (x)\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    \"\"\"\n",
    "    # Dense layers\n",
    "    x = tf.keras.layers.Dense(256, activation='leaky_relu')(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='leaky_relu')(x)\"\"\"\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# Loss functions and optimizers\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "@tf.function(experimental_compile=True)\n",
    "def regularized_generator_loss(fake_output, generated_params, param_averages, lambda_reg=0.1):\n",
    "    # Original GAN loss\n",
    "    gan_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output, from_logits=True))\n",
    "    \n",
    "    # Regularization term\n",
    "    reg_loss = tf.reduce_mean(tf.square(generated_params - param_averages))\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = gan_loss + lambda_reg * reg_loss\n",
    "    \n",
    "    return total_loss #, gan_loss, reg_loss\n",
    "\n",
    "@tf.function(experimental_compile=True)\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "    \n",
    "@tf.function(experimental_compile=True)\n",
    "def generator_loss(fake_output):\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output, from_logits=True))\n",
    "\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650aff59-fa3f-450c-a4d1-0ed0da045e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterMovingAverage:\n",
    "    def __init__(self, decay=0.99):\n",
    "        self.decay = decay\n",
    "        self.averages = None\n",
    "\n",
    "    def update(self, new_params):\n",
    "        if self.averages is None:\n",
    "            self.averages = new_params\n",
    "        else:\n",
    "            self.averages = self.decay * self.averages + (1 - self.decay) * new_params\n",
    "\n",
    "    def get_averages(self):\n",
    "        return self.averages\n",
    "\n",
    "# Initialize the tracker\n",
    "param_tracker = ParameterMovingAverage()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6ca5d-2ce2-44c3-b9c1-8be2c38ef6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function(experimental_compile=True)\n",
    "def train_step(real_data):\n",
    "    param_averages = param_tracker.get_averages()\n",
    "    # tf.random.set_seed(123)\n",
    "    noise = tf.random.normal([batch_size, gen_input_len])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_params = generator(noise, training=True)\n",
    "        # Update the moving averages\n",
    "        \n",
    "        try:\n",
    "            generated_data = tf.map_fn(\n",
    "                sim_channel, \n",
    "                generated_params, \n",
    "                fn_output_signature=tf.float32,\n",
    "                parallel_iterations=1  # This can help with TensorArray issues\n",
    "            )\n",
    "            generated_data = tf.ensure_shape(generated_data, [batch_size, size, 2])\n",
    "        except Exception as e:\n",
    "            tf.print(\"sim_channel error:\", e)\n",
    "            return tf.constant(0, dtype=tf.float32), tf.constant(0, dtype=tf.float32)\n",
    "        \n",
    "        real_output = discriminator(real_data, training=True)\n",
    "        fake_output = discriminator(generated_data, training=True)\n",
    "        \n",
    "        # gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "        gen_loss  = regularized_generator_loss(fake_output, generated_params, param_averages)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    # return gen_loss, disc_loss, gan_loss, reg_loss\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# Training loop\n",
    "@tf.function(experimental_compile=True)\n",
    "def train(dataset, epochs):\n",
    "    steps_per_epoch = math.floor(num_samples / batch_size)\n",
    "    #steps_per_epoch =2\n",
    "    tf.print(\"steps per epoch\", steps_per_epoch)\n",
    "    for epoch in range(getREALepoch(),epochs,1):\n",
    "        EgNoise = tf.random.normal([2, gen_input_len])\n",
    "        #print(\"EgNoise\", EgNoise[0])\n",
    "        generated_params = generator(EgNoise, training=False)\n",
    "        param_tracker.update(tf.reduce_mean(generated_params, axis=0))\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        read_set_lr()\n",
    "        # Initialize loss accumulators for each epoch\n",
    "        epoch_gen_loss = 0\n",
    "        epoch_disc_loss = 0\n",
    "        for step, batch in tqdm(enumerate(dataset.take(steps_per_epoch)), total=steps_per_epoch, ncols=60):\n",
    "            #clear_output(wait=True)\n",
    "            if step >= steps_per_epoch:\n",
    "                break  # Move to the next epoch          \n",
    "            try:\n",
    "                gen_loss, disc_loss = train_step(batch)\n",
    "                #tf.print(\"gen_loss\",gen_loss)\n",
    "                epoch_gen_loss += gen_loss\n",
    "                #tf.print(\"epoch_gen_loss\",gen_loss)\n",
    "                epoch_disc_loss += disc_loss\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training: {e}\")\n",
    "                break\n",
    "\n",
    "        #tf.print(\"epoch_gen_loss\",epoch_gen_loss)\n",
    "        # Calculate average losses for the epoch\n",
    "        avg_gen_loss = epoch_gen_loss / steps_per_epoch\n",
    "        avg_disc_loss = epoch_disc_loss / steps_per_epoch\n",
    "        clear_output(wait=True)\n",
    "        tf.print(f\"Epoch {epoch + 1}/{epochs} - \"\n",
    "              f\"Generator Loss: {avg_gen_loss:.8f}, \"\n",
    "              f\"Discriminator Loss: {avg_disc_loss:.8f}\")\n",
    "        checkpoint.save(file_prefix = 'markovCheckpoints/checkpoint')\n",
    "        # Generate and plot sine waves\n",
    "        egs=2\n",
    "        \n",
    "        # Generate and plot data\n",
    "        EgNoise = tf.random.normal([egs, gen_input_len])\n",
    "        #print(\"EgNoise\", EgNoise[0])\n",
    "        generated_params = generator(EgNoise, training=False)\n",
    "        #print(steps_per_epoch)\n",
    "        # Define parameter names pc12, pc21, relaxation, Fnoise, scale, offset, relaxT, pco1, poc2, po12, po21\n",
    "        param_names = ['pc12', 'pc21', 'relaxation', 'Fnoise', 'scale',\"offset\",\"relaxT\", \"pco1\", \"poc2\",\"po12\",\"po21\"]\n",
    "        random_index1 = tf.random.uniform(shape=[], minval=0, maxval=egs-1, dtype=tf.int32)\n",
    "\n",
    "        params_list = generated_params[random_index1].numpy().tolist()\n",
    "        #Might be fun to collect these up to plot convergence if wanted?\n",
    "        indices = [0,1,7,8,9,10]\n",
    "        p_params = [params_list[i] for i in indices]\n",
    "        \n",
    "        Q = prob_to_rate(p_params, dt)\n",
    "        # Append to CSV\n",
    "        append_rates_to_csv(Q, filename=\"resultsv9.csv\")\n",
    "        \n",
    "        # Print each parameter with its name\n",
    "        with tf.io.gfile.GFile('output.csv', mode='a') as file:\n",
    "            # Check if the file is empty to write the header\n",
    "            if file.tell() == 0:\n",
    "                file.write(','.join(param_names) + '\\n')\n",
    "\n",
    "            for name, param in zip(param_names, params_list):\n",
    "                tf.print(f\"{name}: {round(param, 2)}|\", end = \" \")\n",
    "                file.write(f\"{name},{round(param, 2)}\\n\")\n",
    "        \"\"\" \n",
    "        gen_waves=[]\n",
    "        for i in range(egs):\n",
    "            gen_waves.append( sim_channel(generated_params[i]) )\"\"\"\n",
    "        \n",
    "        random_index2 = tf.random.uniform(shape=[], minval=0, maxval=tf.shape(training_data)[0], dtype=tf.int32)\n",
    "        \"\"\"\n",
    "        biPlotter([gen_waves[random_index1],training_data[random_index2]], random_index1, random_index2, epoch)\n",
    "        \"\"\"\n",
    "        gen_wave = sim_channel(param_tracker.get_averages())\n",
    "        biPlotter([gen_wave,training_data[random_index2]], 0, random_index2, epoch)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Completed {epoch + 1} epochs\")\n",
    "        if writeNow:\n",
    "            writeMe(epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f5189-394e-4b9f-ac37-44adc8341983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biPlotter(data, n, m, epoch):\n",
    "    # Create a figure with two subplots (panels)\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 6))\n",
    "    axs[0].plot(data[0] )\n",
    "    axs[0].set_title(f\"Generated Wave, record {n}, epoch {epoch}\")\n",
    "    axs[0].set_ylim([-1,1.2])\n",
    "    axs[1].plot(data[1] )\n",
    "    axs[1].set_title(f\"Training Data, record {m}\")\n",
    "    axs[1].set_ylim([-1,1.2])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"chanFigs/fig{epoch}.png\")\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe44148-0908-4737-9dff-162920f8d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeMe(samples=100, dt=0.1, epoch=0, file=\"markovData/output.parquet\"):\n",
    "    sampleNoise = tf.random.normal([samples, gen_input_len])\n",
    "    generated_params = generator(sampleNoise, training=False)\n",
    "    gen_waves=[]\n",
    "    for i in range(samples):\n",
    "        gen_waves.extend( sim_channel(generated_params[i]) )\n",
    "    df = pd.DataFrame(gen_waves, columns=[\"Channels\", \"Noisy Current\"])\n",
    "    df[\"Time\"] = dt * pd.Series(range(len(df)))\n",
    "    df = df[[\"Time\", \"Channels\", \"Noisy Current\"]]\n",
    "    df.to_parquet(f\"{epoch}_{file}\")\n",
    "    print(f\"Data saved to {epoch}_{file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b7d3a-36e7-4b71-9bbf-f1c5236672ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getREALepoch() -> int:\n",
    "    import glob\n",
    "    import os\n",
    "    import re\n",
    "    \"\"\"\n",
    "    Save the current image to the working directory of the program.\n",
    "    \"\"\"\n",
    "    currentfiles = glob.glob(\"markovCheckpoints/*.index\")\n",
    "   \n",
    "    numList = [0]\n",
    "    for file in currentfiles:\n",
    "        i = os.path.splitext(file)[0]\n",
    "        try:\n",
    "            pattern = r'-(\\d+)'\n",
    "            num = re.findall(pattern, i)[0]\n",
    "            numList.append(int(num))\n",
    "        except IndexError:\n",
    "            pass\n",
    "    numList = sorted(numList)\n",
    "    return numList[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62630e-8587-4384-b04b-bb9dc76b296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\"\"\"training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    training_data).shuffle(5000).batch(batch_size, drop_remainder=True).repeat()\"\"\"\n",
    "buffer_size = 5000  # Keeping your current shuffle buffer size\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(training_data)\n",
    "training_dataset = (training_dataset\n",
    "    .cache()  # Cache the dataset in memory\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "    .prefetch(tf.data.AUTOTUNE)  # Prefetch next batch while current batch is being processed\n",
    "    .repeat()  # Repeat the dataset indefinitely\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61330f0f-4fb5-449b-b2d9-bd9a94667257",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_compile=True)\n",
    "def read_set_lr():\n",
    "    with open(\"lr.txt\", \"r+\") as my_file:\n",
    "        data = my_file.read()\n",
    "        split = data.split('\\n')\n",
    "        parse_lr_from_file = lambda string: float(string.split(\":\")[1])\n",
    "        new_gen_lr = parse_lr_from_file(split[1])\n",
    "        new_disc_lr = parse_lr_from_file(split[2])\n",
    "        generator_optimizer.learning_rate.assign(tf.cast(new_gen_lr, generator_optimizer.learning_rate.dtype))\n",
    "        discriminator_optimizer.learning_rate.assign(tf.cast(new_disc_lr, discriminator_optimizer.learning_rate.dtype))\n",
    "        tf.print(f\"dLR: {discriminator_optimizer.learning_rate.value.numpy():.3e},\\\n",
    "                    gLR: {generator_optimizer.learning_rate.value.numpy():.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3f1b7-2da8-4556-a693-a954734adc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_value = 1e-5\n",
    "\n",
    "# Convert the learning rate value to the appropriate dtype\n",
    "generator_optimizer.learning_rate.assign(tf.cast(learning_rate_value, generator_optimizer.learning_rate.dtype))\n",
    "learning_rate_value = 1e-4\n",
    "discriminator_optimizer.learning_rate.assign(tf.cast(learning_rate_value, discriminator_optimizer.learning_rate.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5465d-f070-48f7-a6c9-e797a95aca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_set_lr()\n",
    "discriminator_optimizer.learning_rate.value\n",
    "generator_optimizer.learning_rate.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b279af-95ef-4f33-8f86-f447e149662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"markovCheckpoints/checkpoints\"\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer, discriminator_optimizer=discriminator_optimizer, generator=generator, discriminator=discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b6205b-88e7-4f38-9dd6-def65ff0c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore('markovCheckpoints/checkpoint-184').assert_existing_objects_matched()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6798b52-836f-4872-827d-3899b77e1cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writeNow=False\n",
    "epochs=20000 \n",
    "train(training_dataset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ec7c0-c563-4084-9199-f9f7cb38f9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537ed0e-dbd9-407e-b226-83236c9a92fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba7ba19-345d-4661-8e85-c0740ddef099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1bd28-58b0-4cfe-8882-493c43ceea28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
