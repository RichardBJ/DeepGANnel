{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd8be84-d9b7-4c77-8cc3-8cce6c6f7a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "tf.config.run_functions_eagerly(True)\n",
    "size=200\n",
    "filterSilence=True\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "Could select only real data with events. Done\n",
    "Import my save.txt routine to optionally write output.\n",
    "Rename the To TC with rate constants K13 K12 K23 etc.\n",
    "Would another state help? I don't think so... 4 states should capture realistic bursting.\n",
    "Could draw the transition matrix as a graph too? Too fancy. User can do this.\n",
    "Noise could be more authentic still. \n",
    "a. Some slow-wave. freq, phase, amp.\n",
    "b. Some open channel noise. tf.boolean_mask ...if open add white noise? could be slow.\n",
    "c. replace noise with Sam noise? was it all TF?\n",
    "d. SPIKE GENERATION SHOULD BE CHANGED, max should be x not abs(x) and number should be abs(x) not be abs(x) +1. X\n",
    "e. pink noise should also be tf.function decorated!! X\n",
    "\n",
    "F:: concerned about whether I have consistent use of dt.  In some places it seems with have an array that is T long. In other places this is called \n",
    "size and sometimes steps are 1 long and others they are df long @@\n",
    "OK ...looks like the only time dt is used is when plotting simulated output.\n",
    "\n",
    "CHANNEL IS LANE 0\n",
    "RAW IS LANE 1\n",
    "\n",
    "RUNNING ON WSL2 Office xx3655 computer with env101. TF version 2.17 I believe.\n",
    "\"\"\"\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # '0' = all logs, '1' = filter out INFO logs, '2' = filter out WARNING logs, '3' = filter out ERROR logs\n",
    "\n",
    "# Suppress other warnings\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# Additional suppression for TensorFlow 2.x\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "\n",
    "# List all physical GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "print(f\"Number of GPUs detected: {len(gpus)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42709ba-2731-4e53-94ef-08b800ec8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1  # Number of channels\n",
    "dt = tf.constant(0.1, dtype=tf.float32)\n",
    "T = tf.constant(size, dtype=tf.int32)  # In sample points :-)\n",
    "#Must be a multiple of 2!!!\n",
    "#Size of channel (relative to the channels so one channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871588c-0a89-4101-9e00-6f44be354925",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_compile=True)\n",
    "def tf_relaxation(binary_sequence, half_life=4.0, relaxation_amount=0.2):\n",
    "    \"\"\"\n",
    "    Apply exponential relaxation to a 1D binary sequence using vectorized TensorFlow operations.\n",
    "    \n",
    "    Args:\n",
    "    binary_sequence: tf.Tensor, shape [time_steps], sequence of 0s and 1s\n",
    "    half_life: float, the half-life of the exponential decay\n",
    "    relaxation_amount: float, the amount of relaxation (positive or negative)\n",
    "    \n",
    "    Returns:\n",
    "    tf.Tensor, shape [time_steps]\n",
    "    \"\"\"\n",
    "    # Convert input to float32\n",
    "    binary_sequence = tf.cast(binary_sequence, tf.float32)\n",
    "    \n",
    "    # Calculate decay rate\n",
    "    decay_rate = tf.math.log(2.0) / half_life\n",
    "    \n",
    "    # Find the indices where steps occur\n",
    "    steps = tf.not_equal(binary_sequence[1:] - binary_sequence[:-1], 0)\n",
    "    step_indices = tf.where(steps)[:, 0]\n",
    "    \n",
    "    # Calculate the time since each step\n",
    "    time_steps = tf.range(tf.shape(binary_sequence)[0], dtype=tf.float32)\n",
    "    time_since_step = time_steps[:, tf.newaxis] - tf.cast(step_indices, tf.float32)\n",
    "    \n",
    "    # Calculate the exponential decay for each step\n",
    "    decay = tf.exp(-decay_rate * tf.maximum(time_since_step, 0.0))\n",
    "    \n",
    "    # Calculate the relaxation effect\n",
    "    step_values = tf.gather(binary_sequence, step_indices + 1) - tf.gather(binary_sequence, step_indices)\n",
    "    relaxation_effect = relaxation_amount * step_values * decay\n",
    "    \n",
    "    # Sum the effects of all steps\n",
    "    total_relaxation = tf.reduce_sum(relaxation_effect, axis=1)\n",
    "    \n",
    "    # Add the relaxation to the original sequence\n",
    "    relaxed_sequence = binary_sequence + total_relaxation\n",
    "    \n",
    "    return relaxed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d74e988-df26-42e4-b4f9-8744dedd8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a test sequence\n",
    "test_sequence = tf.constant([0, 0, 1, 1, 1, 1, 0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,0,0,0], dtype=tf.float32)\n",
    "\n",
    "# Apply the relaxation\n",
    "relaxed_sequence = tf_relaxation(test_sequence, relaxation_amount=-0.5)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(test_sequence.numpy(), label='Original')\n",
    "plt.plot(relaxed_sequence.numpy(), label='Relaxed')\n",
    "plt.legend()\n",
    "plt.title('Exponential Relaxation')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5426666-b59b-49f1-98bf-55ddfd3c7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "And entirely replaced function, by Claude now that doesn't use tf.probability\n",
    "it seems. Is it really the same?\n",
    "This needs to be checked. Certainly much faster this way. About 5x faster... it is getting 10% of the GPU cuda use now. was barely visible before.\n",
    "\"\"\"\n",
    "@tf.function(experimental_compile=True)\n",
    "def sim_channel(params):\n",
    "    kc12, kc21, relaxation, Fnoise, scale, offset, relaxT, kco1, koc2, ko12, ko21 = params\n",
    "    zero = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "    # Markov chain simulation\n",
    "    \n",
    "    row1 = tf.stack([1-kc12, kc12, zero, zero])\n",
    "    row2 = tf.stack([kc21, 1-kc21-kco1, kco1, zero])\n",
    "    row3 = tf.stack([zero, koc2, 1-koc2-ko12, ko12])\n",
    "    row4 = tf.stack([zero, zero, ko21, 1-ko21])\n",
    "    \n",
    "    transition_matrix = tf.stack([row1, row2, row3, row4])\n",
    "    \n",
    "    # Initial state distribution\n",
    "    initial_probs = tf.constant([0.3, 0.3, 0.2, 0.2])\n",
    "    \n",
    "    # Manual Markov chain simulation\n",
    "    tf.random.set_seed(int(time.time() * 1000) % (2**31 - 1))\n",
    "    def body(i, state, channels):\n",
    "        next_state_probs = tf.gather(transition_matrix, state)\n",
    "        next_state = tf.random.categorical(tf.math.log([next_state_probs]), num_samples=1)[0, 0]\n",
    "        channels = channels.write(i, tf.cast(tf.greater_equal(next_state, 2), tf.float32))\n",
    "        return i+1, next_state, channels\n",
    "\n",
    "    initial_state = tf.random.categorical(tf.math.log([initial_probs]), num_samples=1)[0, 0]\n",
    "    channels = tf.TensorArray(tf.float32, size=T)\n",
    "    _, _, channels = tf.while_loop(\n",
    "        lambda i, *_: i < T,\n",
    "        body,\n",
    "        (0, initial_state, channels)\n",
    "    )\n",
    "    \n",
    "    channels = channels.stack()\n",
    "    channels = tf.squeeze(channels)\n",
    "\n",
    "    # Generate pink noise\n",
    "    white_noise = tf.random.normal(shape=[T])\n",
    "    fft_len = T // 2 + 1\n",
    "    f = tf.range(1, fft_len, dtype=tf.float32)\n",
    "    spectrum = 1.0 / tf.sqrt(f)\n",
    "    spectrum = tf.concat([tf.constant([1.0]), spectrum], axis=0)\n",
    "    white_noise_fft = tf.signal.rfft(white_noise)\n",
    "    pink_noise_fft = white_noise_fft * tf.cast(spectrum, tf.complex64)\n",
    "    pink_noise = tf.signal.irfft(pink_noise_fft)\n",
    "    pink_noise -= tf.reduce_mean(pink_noise)\n",
    "    pink_noise = pink_noise / tf.math.reduce_std(pink_noise)\n",
    "    noise = pink_noise * Fnoise\n",
    "\n",
    "    # Add relaxation\n",
    "    modified_raw_column = (channels * scale) + offset\n",
    "    modified_raw_column = tf_relaxation(modified_raw_column, half_life=relaxT, relaxation_amount=relaxation)\n",
    "    \n",
    "    modified_raw_column += noise\n",
    "\n",
    "    # Combine channels and modified raw column\n",
    "    image = tf.stack([channels, modified_raw_column], axis=1)\n",
    "    \n",
    "    # Final safeguard against NaN values\n",
    "    image = tf.where(tf.math.is_nan(image), tf.zeros_like(image), image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322af487-6e57-4de2-8d21-0d99711234c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the exponential distribution\n",
    "num_samples = 5\n",
    "\"\"\"kc12, kc21, relaxation, Fnoise, scale, offset, relaxT, kco1, koc2, ko12, ko21\"\"\"\n",
    "kc12 = tf.constant(0.1, dtype=tf.float32)  # Adjust this value as needed\n",
    "kc21 = tf.constant(0.1, dtype=tf.float32)  # Adjust this value as needed\n",
    "kco1 = tf.constant(0.01, dtype=tf.float32)\n",
    "koc2 = tf.constant(0.01, dtype=tf.float32)\n",
    "ko12 = tf.constant(0.01, dtype=tf.float32)\n",
    "ko21 = tf.constant(0.01, dtype=tf.float32)\n",
    "\n",
    "\n",
    "relaxation = tf.constant(0.5, dtype=tf.float32)\n",
    "Fnoise = tf.constant(.04, dtype=tf.float32)\n",
    "SCALE = tf.constant(.6, dtype=tf.float32)\n",
    "#And an offset\n",
    "OFFSET = tf.constant(-0.4, dtype=tf.float32)\n",
    "relaxT=25\n",
    "# nE = tf.constant(200, dtype=tf.int32) #number of events\n",
    "\n",
    "\n",
    "# Generate training data\n",
    "training_data = []\n",
    "lens=[]\n",
    "\"\"\"Actually replace \"Anoise\" with relaxation later\"\"\"\n",
    "for sample in tqdm(range(num_samples)):   \n",
    "    params = tf.stack([kc12, kc21, relaxation, Fnoise, SCALE, OFFSET, relaxT, kco1, koc2, ko12,ko21])  # Use tf.stack instead of tf.constant\n",
    "    segment = sim_channel(params)\n",
    "    lens.append(sum(abs(segment)))\n",
    "    training_data.append(segment)\n",
    "print(f\"Average duration was {sum(lens)/len(lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fd338-3df9-4a62-91e1-f5a26648b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(data):\n",
    "    # Create a figure with two subplots (panels)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 6))\n",
    "     # Flatten the axs array for easy iteration\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for i in range(4):\n",
    "        axs[i].plot(data[i])\n",
    "        #axs[i].set_ylim([-200, 200])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotter(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea8ea2-3420-4a42-8df7-f9361a049138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create REAL Trainging Data\n",
    "file_path = \"Lina2/4096lina11raw.csv\"\n",
    "df = pd.read_csv(file_path, header=None, names=[\"Raw\", \"Channels\"])\n",
    "df = df[[\"Channels\",\"Raw\"]]\n",
    "# now crop to just one phenotype. There seem multiple in this dataset.\n",
    "#df=df[:75000]\n",
    "#df=df[:12000]\n",
    "df = pd.concat([df] * 5, ignore_index=True)\n",
    "noise = np.random.normal(0, 0.01, df[\"Raw\"].shape)\n",
    "df[\"Raw\"] += noise\n",
    "num_rows = (len(df) // size) * size\n",
    "print(num_rows)\n",
    "df = df.iloc[:num_rows]\n",
    "data_array = df.to_numpy()\n",
    "data_tensor = tf.convert_to_tensor(data_array, dtype=tf.float32)\n",
    "training_data = tf.reshape(data_tensor, [-1, size, 2])\n",
    "#Calculate real num_samples!\n",
    "num_samples= tf.shape(training_data)[0]\n",
    "#num_samples = 10 #debug\n",
    "#Only use windows where something happened!\n",
    "filterSilence = True\n",
    "if filterSilence:\n",
    "    first_column = training_data[:, :, 0]\n",
    "    all_same = tf.reduce_all(tf.equal(first_column, first_column[:, 0:1]), axis=1)\n",
    "    \n",
    "    # Filter out batches where all values in the first column are the same\n",
    "    training_data = tf.boolean_mask(training_data, ~all_same)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280dab93-b2e1-4214-b409-ca5f619e2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Raw\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a169f4-e9e4-4000-9a30-ba0653f109a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator model\n",
    "\"\"\"kc12, kc21, relaxation, Fnoise, SCALE, OFFSET, relaxT, kco1, koc2, ko12, ko21\"\"\"\n",
    "gen_input_len=11\n",
    "\n",
    "def make_generator_model():\n",
    "    \"\"\"batch normalisation is terrible!!\"\"\"\n",
    "    noise_input = tf.keras.layers.Input(shape=(gen_input_len,))\n",
    "    x = tf.keras.layers.Dense(128)(noise_input)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.Dense(256)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    # Output layer without activation\n",
    "    raw_output = tf.keras.layers.Dense(gen_input_len)(x)\n",
    "    \n",
    "    # Apply appropriate activations/scaling to each output\n",
    "    kc12 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 1e-6, 1e-6, 1.0))(raw_output[:, 0:1])\n",
    "    kc21 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 1e-6, 1e-6, 1.0))(raw_output[:, 1:2])\n",
    "    relaxation = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1.0, 1.0))(raw_output[:, 2:3])\n",
    "    Fnoise = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x), 0.0, 1.0))(raw_output[:, 3:4])\n",
    "    scale = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 0.1, 0.1, 10.0))(raw_output[:, 4:5])\n",
    "    offset = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(x, -1.0, 1.0))(raw_output[:, 5:6])\n",
    "    relaxT = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) * 100, 1.0, size - 1.0))(raw_output[:, 6:7])\n",
    "    kco1 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 1e-3, 1e-6, 1.0))(raw_output[:, 7:8])\n",
    "    koc2 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 1e-6, 1e-6, 1.0))(raw_output[:, 8:9])\n",
    "    ko12 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 1e-6, 1e-6, 1.0))(raw_output[:, 9:10])\n",
    "    ko21 = tf.keras.layers.Lambda(lambda x: tf.clip_by_value(tf.abs(x) + 1e-6, 1e-6, 1.0))(raw_output[:, 10:11])\n",
    "    \n",
    "    output = tf.keras.layers.Concatenate()([kc12, kc21, relaxation, Fnoise, scale, offset, relaxT, kco1, koc2, ko12, ko21])\n",
    "    \n",
    "    return tf.keras.Model(inputs=noise_input, outputs=output)\n",
    "\n",
    "# Define the discriminator model batch, record len, channels = events then noise\n",
    "num_points = T.numpy().item()\n",
    "def make_discriminator_model():\n",
    "    input_shape = (size,2) \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Reshape input to add channel dimension\n",
    "    x = tf.keras.layers.Reshape((size, 2))(inputs)\n",
    "    \n",
    "    # 1D Convolutional layers\n",
    "    x = tf.keras.layers.Conv1D(8, kernel_size=5, strides=2, padding='same', activation='leaky_relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(16, kernel_size=5, strides=2, padding='same', activation='leaky_relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(32, kernel_size=5, strides=2, padding='same', activation='leaky_relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3) (x)\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    \"\"\"\n",
    "    # Dense layers\n",
    "    x = tf.keras.layers.Dense(256, activation='leaky_relu')(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='leaky_relu')(x)\"\"\"\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# Loss functions and optimizers\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output, from_logits=True))\n",
    "\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6ca5d-2ce2-44c3-b9c1-8be2c38ef6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function(experimental_compile=True)\n",
    "def train_step(real_data):\n",
    "    tf.random.set_seed(123)\n",
    "    noise = tf.random.normal([batch_size, gen_input_len])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_params = generator(noise, training=True)\n",
    "\n",
    "        try:\n",
    "            generated_data = tf.map_fn(\n",
    "                sim_channel, \n",
    "                generated_params, \n",
    "                fn_output_signature=tf.float32,\n",
    "                parallel_iterations=1  # This can help with TensorArray issues\n",
    "            )\n",
    "            generated_data = tf.ensure_shape(generated_data, [batch_size, size, 2])\n",
    "        except Exception as e:\n",
    "            tf.print(\"sim_channel error:\", e)\n",
    "            return tf.constant(0, dtype=tf.float32), tf.constant(0, dtype=tf.float32)\n",
    "        \n",
    "        real_output = discriminator(real_data, training=True)\n",
    "        fake_output = discriminator(generated_data, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# Training loop\n",
    "@tf.function(experimental_compile=True)\n",
    "def train(dataset, epochs):\n",
    "    steps_per_epoch = math.floor(num_samples / batch_size)\n",
    "    #steps_per_epoch =2\n",
    "    tf.print(\"steps per epoch\", steps_per_epoch)\n",
    "    for epoch in range(getREALepoch(),epochs,1):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        read_set_lr()\n",
    "        # Initialize loss accumulators for each epoch\n",
    "        epoch_gen_loss = 0\n",
    "        epoch_disc_loss = 0\n",
    "        \n",
    "        for step, batch in tqdm(enumerate(dataset), total=steps_per_epoch, ncols=60):\n",
    "            #clear_output(wait=True)\n",
    "            if step >= steps_per_epoch:\n",
    "                break  # Move to the next epoch          \n",
    "            try:\n",
    "                gen_loss, disc_loss = train_step(batch)\n",
    "                #tf.print(\"gen_loss\",gen_loss)\n",
    "                epoch_gen_loss += gen_loss\n",
    "                #tf.print(\"epoch_gen_loss\",gen_loss)\n",
    "                epoch_disc_loss += disc_loss\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training: {e}\")\n",
    "                break\n",
    "\n",
    "        #tf.print(\"epoch_gen_loss\",epoch_gen_loss)\n",
    "        # Calculate average losses for the epoch\n",
    "        avg_gen_loss = epoch_gen_loss / steps_per_epoch\n",
    "        avg_disc_loss = epoch_disc_loss / steps_per_epoch\n",
    "        clear_output(wait=True)\n",
    "        tf.print(f\"Epoch {epoch + 1}/{epochs} - \"\n",
    "              f\"Generator Loss: {avg_gen_loss:.8f}, \"\n",
    "              f\"Discriminator Loss: {avg_disc_loss:.8f}\")\n",
    "        checkpoint.save(file_prefix = 'markovCheckpoints/checkpoint')\n",
    "        # Generate and plot sine waves\n",
    "        egs=2\n",
    "        \n",
    "        # Generate and plot data\n",
    "        EgNoise = tf.random.normal([egs, gen_input_len])\n",
    "        #print(\"EgNoise\", EgNoise[0])\n",
    "        generated_params = generator(EgNoise, training=False)\n",
    "        #print(steps_per_epoch)\n",
    "        # Define parameter names kc12, kc21, relaxation, Fnoise, scale, offset, relaxT, kco1, koc2, ko12, ko21\n",
    "        param_names = ['kc12', 'kc21', 'relaxation', 'Fnoise', 'scale',\"offset\",\"relaxT\", \"kco1\", \"koc2\",\"ko12\",\"ko21\"]\n",
    "        random_index1 = tf.random.uniform(shape=[], minval=0, maxval=egs-1, dtype=tf.int32)\n",
    "\n",
    "        params_list = generated_params[random_index1].numpy().tolist()\n",
    "        #Might be fun to collect these up to plot convergence if wanted?\n",
    "                \n",
    "        # Print each parameter with its name\n",
    "        with tf.io.gfile.GFile('output.csv', mode='a') as file:\n",
    "            # Check if the file is empty to write the header\n",
    "            if file.tell() == 0:\n",
    "                file.write(','.join(param_names) + '\\n')\n",
    "\n",
    "            for name, param in zip(param_names, params_list):\n",
    "                tf.print(f\"{name}: {round(param, 2)}|\", end = \" \")\n",
    "                file.write(f\"{name},{round(param, 2)}\\n\")\n",
    "         \n",
    "        gen_waves=[]\n",
    "        for i in range(egs):\n",
    "            gen_waves.append( sim_channel(generated_params[i]) )\n",
    "\n",
    "        \"\"\"\n",
    "        # Create a figure with two subplots (panels)\n",
    "        fig, axs = plt.subplots(egs, 1, figsize=(10, 6))\n",
    "        for i in range(egs):\n",
    "            axs[i].plot(gen_waves[i] )\n",
    "            #axs[i].set_ylim([0,1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        #tf.print(tf.shape(gen_waves))\n",
    "        random_index2 = tf.random.uniform(shape=[], minval=0, maxval=tf.shape(training_data)[0], dtype=tf.int32)\n",
    "\n",
    "        biPlotter([gen_waves[random_index1],training_data[random_index2]], random_index1, random_index2, epoch)\n",
    "              \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Completed {epoch + 1} epochs\")\n",
    "        if writeNow:\n",
    "            writeMe(epoch=epoch)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f5189-394e-4b9f-ac37-44adc8341983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biPlotter(data, n, m, epoch):\n",
    "    # Create a figure with two subplots (panels)\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 6))\n",
    "    axs[0].plot(data[0] )\n",
    "    axs[0].set_title(f\"Generated Wave, record {n}, epoch {epoch}\")\n",
    "    axs[0].set_ylim([-1,1.2])\n",
    "    axs[1].plot(data[1] )\n",
    "    axs[1].set_title(f\"Training Data, record {m}\")\n",
    "    axs[1].set_ylim([-1,1.2])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"chanFigs/fig{epoch}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe44148-0908-4737-9dff-162920f8d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeMe(samples=100, dt=0.1, epoch=0, file=\"markovData/output.parquet\"):\n",
    "    sampleNoise = tf.random.normal([samples, gen_input_len])\n",
    "    generated_params = generator(sampleNoise, training=False)\n",
    "    gen_waves=[]\n",
    "    for i in range(samples):\n",
    "        gen_waves.extend( sim_channel(generated_params[i]) )\n",
    "    df = pd.DataFrame(gen_waves, columns=[\"Channels\", \"Noisy Current\"])\n",
    "    df[\"Time\"] = dt * pd.Series(range(len(df)))\n",
    "    df = df[[\"Time\", \"Channels\", \"Noisy Current\"]]\n",
    "    df.to_parquet(f\"{epoch}_{file}\")\n",
    "    print(f\"Data saved to {epoch}_{file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b7d3a-36e7-4b71-9bbf-f1c5236672ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getREALepoch() -> int:\n",
    "    import glob\n",
    "    import os\n",
    "    import re\n",
    "    \"\"\"\n",
    "    Save the current image to the working directory of the program.\n",
    "    \"\"\"\n",
    "    currentfiles = glob.glob(\"markovCheckpoints/*.index\")\n",
    "   \n",
    "    numList = [0]\n",
    "    for file in currentfiles:\n",
    "        i = os.path.splitext(file)[0]\n",
    "        try:\n",
    "            pattern = r'-(\\d+)'\n",
    "            num = re.findall(pattern, i)[0]\n",
    "            numList.append(int(num))\n",
    "        except IndexError:\n",
    "            pass\n",
    "    numList = sorted(numList)\n",
    "    return numList[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62630e-8587-4384-b04b-bb9dc76b296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    training_data).shuffle(5000).batch(batch_size, drop_remainder=True).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61330f0f-4fb5-449b-b2d9-bd9a94667257",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_compile=True)\n",
    "def read_set_lr():\n",
    "    with open(\"lr.txt\", \"r+\") as my_file:\n",
    "        data = my_file.read()\n",
    "        split = data.split('\\n')\n",
    "        parse_lr_from_file = lambda string: float(string.split(\":\")[1])\n",
    "        new_gen_lr = parse_lr_from_file(split[1])\n",
    "        new_disc_lr = parse_lr_from_file(split[2])\n",
    "        generator_optimizer.learning_rate.assign(tf.cast(new_gen_lr, generator_optimizer.learning_rate.dtype))\n",
    "        discriminator_optimizer.learning_rate.assign(tf.cast(new_disc_lr, discriminator_optimizer.learning_rate.dtype))\n",
    "        tf.print(f\"dLR: {discriminator_optimizer.learning_rate.value.numpy():.3e},\\\n",
    "                    gLR: {generator_optimizer.learning_rate.value.numpy():.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3f1b7-2da8-4556-a693-a954734adc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_value = 1e-5\n",
    "\n",
    "# Convert the learning rate value to the appropriate dtype\n",
    "generator_optimizer.learning_rate.assign(tf.cast(learning_rate_value, generator_optimizer.learning_rate.dtype))\n",
    "learning_rate_value = 1e-4\n",
    "discriminator_optimizer.learning_rate.assign(tf.cast(learning_rate_value, discriminator_optimizer.learning_rate.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5465d-f070-48f7-a6c9-e797a95aca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_set_lr()\n",
    "discriminator_optimizer.learning_rate.value\n",
    "generator_optimizer.learning_rate.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b279af-95ef-4f33-8f86-f447e149662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"markovCheckpoints/checkpoints\"\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer, discriminator_optimizer=discriminator_optimizer, generator=generator, discriminator=discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6798b52-836f-4872-827d-3899b77e1cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writeNow=False\n",
    "epochs=20000\n",
    "train(training_dataset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b6205b-88e7-4f38-9dd6-def65ff0c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore('markovCheckpoints/checkpoint-78').assert_existing_objects_matched()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ec7c0-c563-4084-9199-f9f7cb38f9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537ed0e-dbd9-407e-b226-83236c9a92fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba7ba19-345d-4661-8e85-c0740ddef099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1bd28-58b0-4cfe-8882-493c43ceea28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
