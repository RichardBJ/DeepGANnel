{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd8be84-d9b7-4c77-8cc3-8cce6c6f7a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 16:28:02.297557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-21 16:28:02.331338: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-21 16:28:02.337272: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-21 16:28:02.358082: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-21 16:28:03.668844: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs detected: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726932486.577592    5944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1726932486.624815    5944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1726932486.624887    5944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "tf.config.run_functions_eagerly(True)\n",
    "size=200\n",
    "filterSilence=True\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "Could select only real data with events. Done\n",
    "Import my save.txt routine to optionally write output.\n",
    "Rename the To TC with rate constants K13 K12 K23 etc.\n",
    "Would another state help? I don't think so... 4 states should capture realistic bursting.\n",
    "Could draw the transition matrix as a graph too? Too fancy. User can do this.\n",
    "Noise could be more authentic still. \n",
    "a. Some slow-wave. freq, phase, amp.\n",
    "b. Some open channel noise. tf.boolean_mask ...if open add white noise? could be slow.\n",
    "c. replace noise with Sam noise? was it all TF?\n",
    "d. SPIKE GENERATION SHOULD BE CHANGED, max should be x not abs(x) and number should be abs(x) not be abs(x) +1. X\n",
    "e. pink noise should also be tf.function decorated!! X\n",
    "\n",
    "CHANNEL IS LANE 0\n",
    "RAW IS LANE 1\n",
    "\n",
    "RUNNING ON NUMAN-NO_GPU in the TF10 envirnoment which means tf2.10!!! not tf1.0 :-)\n",
    "\"\"\"\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # '0' = all logs, '1' = filter out INFO logs, '2' = filter out WARNING logs, '3' = filter out ERROR logs\n",
    "\n",
    "# Suppress other warnings\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# Additional suppression for TensorFlow 2.x\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# List all physical GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "print(f\"Number of GPUs detected: {len(gpus)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "582141b3-8420-4f6c-99e6-f8249714c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def spike_up(signal, num_spikes=10, minh=-0.1, maxh=0.1):\n",
    "    if maxh < 0:\n",
    "        minh=maxh\n",
    "    else:\n",
    "        minh=0\n",
    "    spike_heights = tf.random.uniform(shape=[num_spikes], minval=-minh, maxval=maxh) \n",
    "    spike_indices = tf.random.uniform(shape=[num_spikes], \n",
    "                                      minval=0, maxval=tf.shape(signal)[0], dtype=tf.int32)\n",
    "    \n",
    "    # Create a tensor of zeros with the same shape as the signal\n",
    "    spike_tensor = tf.zeros_like(signal) \n",
    "    \n",
    "    # Set the values at the spike indices\n",
    "    spike_tensor = tf.tensor_scatter_nd_update(spike_tensor, tf.expand_dims(spike_indices, axis=-1), spike_heights)\n",
    "    \n",
    "    # Add the spikes to the signal\n",
    "    return signal + spike_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42709ba-2731-4e53-94ef-08b800ec8b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1726932486.822977    5944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1726932486.823106    5944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1726932486.823137    5944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1726932487.190650    5944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1726932487.190756    5944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-21 16:28:07.190770: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1726932487.190821    5944 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:15:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-21 16:28:07.190842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22277 MB memory:  -> device: 0, name: NVIDIA TITAN RTX, pci bus id: 0000:15:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "n = 1  # Number of channels\n",
    "dt = tf.constant(0.1, dtype=tf.float32)\n",
    "T = tf.constant(size, dtype=tf.int32)  # In sample points :-)\n",
    "#Must be a multiple of 2!!!\n",
    "#Size of channel (relative to the channels so one channel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94f1599a-b47b-46a2-9480-d3b3610502d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sim_channel(params):\n",
    "    # ([To, Tc1, Anoise, Fnoise, scale, offset, nSpikes, Tc2, To2])\n",
    "    kc12, kc21, spikeMax, Fnoise, scale, offset, nSpikes, kco1, koc2, ko12, ko21 = params\n",
    "\n",
    "    \"\"\"\n",
    "    X  C1  C2  O1  O2\n",
    "    C1  Z        \n",
    "    C2      Z\n",
    "    O1          Z\n",
    "    O2              Z\n",
    "\n",
    "    \"\"\"\n",
    "    #nE = tf.cast(nE, dtype=tf.int32)\n",
    "    nSpikes = tf.cast(nSpikes, dtype=tf.int32)\n",
    "    t = tf.range(0, T, dtype=tf.float32) * dt\n",
    "    zero = tf.constant(0.0, dtype=tf.float32)\n",
    "    row1 = tf.stack([zero, kc12, zero, zero])\n",
    "    row2 = tf.stack([kc21, zero, kco1, zero])\n",
    "    row3 = tf.stack([zero, koc2, zero, ko12])\n",
    "    row4 = tf.stack([zero,zero,ko21,zero])\n",
    "    r1 = tf.reduce_sum(row1)\n",
    "    row1 = tf.tensor_scatter_nd_update(row1, [[0]], [1 - r1])\n",
    "\n",
    "    r2 = tf.reduce_sum(row2)\n",
    "    row2 = tf.tensor_scatter_nd_update(row2, [[1]], [1 - r2])\n",
    "    \n",
    "    r3 = tf.reduce_sum(row3)\n",
    "    row3 = tf.tensor_scatter_nd_update(row3, [[2]], [1 - r3])\n",
    "    \n",
    "    r4 = tf.reduce_sum(row4)\n",
    "    row4 = tf.tensor_scatter_nd_update(row4, [[3]], [1 - r4])\n",
    "\n",
    "    \n",
    "    transition_matrix = tf.stack([row1, row2, row3, row4])\n",
    "    \n",
    "    # Define the transition function\n",
    "    # This function uses tf.gather to select the correct row of the transition matrix\n",
    "    transition_fn = lambda _, state: tfp.distributions.Categorical(\n",
    "        probs=tf.gather(transition_matrix, state))  \n",
    "    \n",
    "    # Define the initial state distribution\n",
    "    initial_distribution = tfp.distributions.Categorical(probs=[0.3, 0.3, 0.2, 0.2])\n",
    "    \n",
    "    # Define the Markov chain\n",
    "    markov_chain = tfp.distributions.MarkovChain(\n",
    "        initial_state_prior=initial_distribution,\n",
    "        transition_fn=transition_fn,\n",
    "        num_steps=T  # Number of steps to simulate\n",
    "    )\n",
    "\n",
    "    # Sample from the Markov chain\n",
    "    channels = markov_chain.sample()\n",
    "\n",
    "    channels = tf.where(tf.less(channels, 2), tf.zeros_like(channels), tf.ones_like(channels))\n",
    "    \n",
    "    # Add noise\n",
    "    tf.random.set_seed(None)\n",
    "    white_noise = tf.random.normal(shape=[T]) #*Anoise\n",
    "    pink_noise = generate_pink_noise(white_noise)*Fnoise\n",
    "    \n",
    "    noise = pink_noise #+white_noise\n",
    "\n",
    "    channels = tf.cast(channels, tf.float32)\n",
    "    noise = tf.cast(noise, tf.float32)\n",
    "    \n",
    "    #image = tf.stack([channels,noise],axis=1)\n",
    "    \n",
    "    # Manipulate the RAW column with offsets and scale\n",
    "    modified_raw_column = (channels * scale) + offset\n",
    "    modified_raw_column = spike_up(modified_raw_column, num_spikes=nSpikes, maxh=spikeMax) + noise\n",
    "\n",
    "    # Concatenate the modified first column with the second column\n",
    "    image = tf.concat([tf.reshape(channels, (-1, 1)), tf.reshape(modified_raw_column, (-1, 1)) ], axis=1)\n",
    "    \"\"\"plt.plot(modified_raw_column)\n",
    "    plt.show()\"\"\"\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb09474-49e4-45d6-983d-3f80e2f10e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def generate_pink_noise(white_noise):\n",
    "    #tf.random.set_seed(None)\n",
    "    T = tf.shape(white_noise)[0]\n",
    "    # Compute the number of unique FFT coefficients\n",
    "    num_fft_pts = T // 2 + 1\n",
    "    \n",
    "    # Generate the frequency spectrum\n",
    "    f = tf.range(1, num_fft_pts, dtype=tf.float32)\n",
    "    spectrum = 1.0 / tf.sqrt(f)\n",
    "    spectrum = tf.concat([tf.constant([1.0]), spectrum], axis=0)\n",
    "    \n",
    "    # Compute FFT of white noise\n",
    "    white_noise_fft = tf.signal.rfft(white_noise)\n",
    "    \n",
    "    # Apply pink noise spectrum\n",
    "    pink_noise_fft = white_noise_fft * tf.cast(spectrum, tf.complex64)\n",
    "    \n",
    "    # Inverse FFT to get pink noise in time domain\n",
    "    pink_noise = tf.signal.irfft(pink_noise_fft)\n",
    "    \n",
    "    # Remove DC offset\n",
    "    pink_noise -= tf.reduce_mean(pink_noise)\n",
    "    \n",
    "    # Normalize\n",
    "    return pink_noise / tf.math.reduce_std(pink_noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "216e6cf7-8224-4fde-b1f4-effa50448aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def tf_relaxation(binary_sequence, half_life=4.0, relaxation_amount=0.2):\n",
    "    \"\"\"\n",
    "    Apply relaxation to a 1D binary sequence using TensorFlow operations.\n",
    "    \n",
    "    Args:\n",
    "    binary_sequence: tf.Tensor, shape [time_steps], sequence of 0s and 1s\n",
    "    half_life: float, the half-life of the exponential decay\n",
    "    relaxation_amount: float, the amount of relaxation (positive or negative)\n",
    "    \n",
    "    Returns:\n",
    "    tf.Tensor, shape [time_steps]\n",
    "    \"\"\"\n",
    "    decay_factor = tf.math.log(2.0) / half_life\n",
    "    \n",
    "    @tf.function\n",
    "    def relaxation_step(state, current_sample):\n",
    "        prev_level, prev_step = state\n",
    "        \n",
    "        # Calculate new step\n",
    "        new_step = current_sample - prev_level\n",
    "        step = tf.where(tf.not_equal(new_step, 0.0), new_step, prev_step)\n",
    "        \n",
    "        # Calculate target level and relaxation\n",
    "        target_level = current_sample + relaxation_amount * step\n",
    "        relaxation = (prev_level - target_level) * tf.exp(-decay_factor)\n",
    "        current_level = target_level + relaxation\n",
    "        \n",
    "        return (current_level, step), current_level\n",
    "\n",
    "    # Prepare initial state\n",
    "    initial_state = (binary_sequence[0], tf.constant(0.0))\n",
    "\n",
    "    # Apply relaxation using scan\n",
    "    relaxed_sequence = tf.scan(\n",
    "        relaxation_step,\n",
    "        binary_sequence,\n",
    "        initializer=initial_state\n",
    "    )\n",
    "\n",
    "    # Return only the relaxed levels\n",
    "    return relaxed_sequence[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5426666-b59b-49f1-98bf-55ddfd3c7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "And entirely replaced function, by Claude now that doesn't use tf.probability\n",
    "it seems. Is it really the same?\n",
    "This needs to be checked. Certainly much faster this way. About 5x faster... it is getting 10% of the GPU cuda use now. was barely visible before.\n",
    "\"\"\"\n",
    "@tf.function(experimental_compile=True)\n",
    "def sim_channel(params):\n",
    "    kc12, kc21, spikeMax, Fnoise, scale, offset, nSpikes, kco1, koc2, ko12, ko21 = params\n",
    "    zero = tf.constant(0.0, dtype=tf.float32)\n",
    "    # swapping spikes for relaxtion now!\n",
    "    nSpikes = tf.cast(nSpikes, tf.float32)\n",
    "    spikeMax = tf.cast(spikeMax, tf.float32)\n",
    "\n",
    "    # Markov chain simulation\n",
    "    \n",
    "    row1 = tf.stack([1-kc12, kc12, zero, zero])\n",
    "    row2 = tf.stack([kc21, 1-kc21-kco1, kco1, zero])\n",
    "    row3 = tf.stack([zero, koc2, 1-koc2-ko12, ko12])\n",
    "    row4 = tf.stack([zero, zero, ko21, 1-ko21])\n",
    "    \n",
    "    transition_matrix = tf.stack([row1, row2, row3, row4])\n",
    "    \n",
    "    # Initial state distribution\n",
    "    initial_probs = tf.constant([0.3, 0.3, 0.2, 0.2])\n",
    "    \n",
    "    # Manual Markov chain simulation\n",
    "    def body(i, state, channels):\n",
    "        next_state_probs = tf.gather(transition_matrix, state)\n",
    "        next_state = tf.random.categorical(tf.math.log([next_state_probs]), num_samples=1)[0, 0]\n",
    "        channels = channels.write(i, tf.cast(tf.greater_equal(next_state, 2), tf.float32))\n",
    "        return i+1, next_state, channels\n",
    "\n",
    "    initial_state = tf.random.categorical(tf.math.log([initial_probs]), num_samples=1)[0, 0]\n",
    "    channels = tf.TensorArray(tf.float32, size=T)\n",
    "    _, _, channels = tf.while_loop(\n",
    "        lambda i, *_: i < T,\n",
    "        body,\n",
    "        (0, initial_state, channels)\n",
    "    )\n",
    "    \n",
    "    channels = channels.stack()\n",
    "    channels = tf.squeeze(channels)\n",
    "\n",
    "    # Generate pink noise\n",
    "    white_noise = tf.random.normal(shape=[T])\n",
    "    fft_len = T // 2 + 1\n",
    "    f = tf.range(1, fft_len, dtype=tf.float32)\n",
    "    spectrum = 1.0 / tf.sqrt(f)\n",
    "    spectrum = tf.concat([tf.constant([1.0]), spectrum], axis=0)\n",
    "    white_noise_fft = tf.signal.rfft(white_noise)\n",
    "    pink_noise_fft = white_noise_fft * tf.cast(spectrum, tf.complex64)\n",
    "    pink_noise = tf.signal.irfft(pink_noise_fft)\n",
    "    pink_noise -= tf.reduce_mean(pink_noise)\n",
    "    pink_noise = pink_noise / tf.math.reduce_std(pink_noise)\n",
    "    noise = pink_noise * Fnoise\n",
    "\n",
    "    # Add relaxation\n",
    "    \n",
    "    modified_raw_column = (channels * scale) + offset\n",
    "    modified_raw_column = tf_relaxation(modified_raw_column, half_life=nSpikes, relaxation_amount=spikeMax)\n",
    "    \n",
    "    modified_raw_column +=  noise\n",
    "\n",
    "    # Combine channels and modified raw column\n",
    "    image = tf.stack([channels, modified_raw_column], axis=1)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "322af487-6e57-4de2-8d21-0d99711234c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                    | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The two structures don't have the same nested structure.\n\nFirst structure: type=tuple str=(<tf.Tensor: shape=(), dtype=float32, numpy=0.15>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)\n\nSecond structure: type=tuple str=((<tf.Tensor: shape=(), dtype=float32, numpy=0.15>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>), <tf.Tensor: shape=(), dtype=float32, numpy=0.15>)\n\nMore specifically: Substructure \"type=tuple str=(<tf.Tensor: shape=(), dtype=float32, numpy=0.15>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)\" is a sequence, while substructure \"type=EagerTensor str=tf.Tensor(0.15, shape=(), dtype=float32)\" is not\nEntire first structure:\n(., .)\nEntire second structure:\n((., .), .)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_samples)):   \n\u001b[1;32m     26\u001b[0m     params \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstack([kc12, kc21, Anoise, Fnoise, SCALE, OFFSET, num_spikes, kco1, koc2, ko12,ko21])  \u001b[38;5;66;03m# Use tf.stack instead of tf.constant\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     segment \u001b[38;5;241m=\u001b[39m sim_channel(params)\n\u001b[1;32m     28\u001b[0m     lens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mabs\u001b[39m(segment)))\n\u001b[1;32m     29\u001b[0m     training_data\u001b[38;5;241m.\u001b[39mappend(segment)\n",
      "File \u001b[0;32m~/miniconda3/envs/env101/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[13], line 60\u001b[0m, in \u001b[0;36msim_channel\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Add relaxation\u001b[39;00m\n\u001b[1;32m     59\u001b[0m modified_raw_column \u001b[38;5;241m=\u001b[39m (channels \u001b[38;5;241m*\u001b[39m scale) \u001b[38;5;241m+\u001b[39m offset\n\u001b[0;32m---> 60\u001b[0m modified_raw_column \u001b[38;5;241m=\u001b[39m tf_relaxation(modified_raw_column, half_life\u001b[38;5;241m=\u001b[39mnSpikes, relaxation_amount\u001b[38;5;241m=\u001b[39mspikeMax)\n\u001b[1;32m     62\u001b[0m modified_raw_column \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m  noise\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Combine channels and modified raw column\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m, in \u001b[0;36mtf_relaxation\u001b[0;34m(binary_sequence, half_life, relaxation_amount)\u001b[0m\n\u001b[1;32m     32\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m (binary_sequence[\u001b[38;5;241m0\u001b[39m], tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m0.0\u001b[39m))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Apply relaxation using scan\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m relaxed_sequence \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mscan(\n\u001b[1;32m     36\u001b[0m     relaxation_step,\n\u001b[1;32m     37\u001b[0m     binary_sequence,\n\u001b[1;32m     38\u001b[0m     initializer\u001b[38;5;241m=\u001b[39minitial_state\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Return only the relaxed levels\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m relaxed_sequence[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The two structures don't have the same nested structure.\n\nFirst structure: type=tuple str=(<tf.Tensor: shape=(), dtype=float32, numpy=0.15>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)\n\nSecond structure: type=tuple str=((<tf.Tensor: shape=(), dtype=float32, numpy=0.15>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>), <tf.Tensor: shape=(), dtype=float32, numpy=0.15>)\n\nMore specifically: Substructure \"type=tuple str=(<tf.Tensor: shape=(), dtype=float32, numpy=0.15>, <tf.Tensor: shape=(), dtype=float32, numpy=0.0>)\" is a sequence, while substructure \"type=EagerTensor str=tf.Tensor(0.15, shape=(), dtype=float32)\" is not\nEntire first structure:\n(., .)\nEntire second structure:\n((., .), .)"
     ]
    }
   ],
   "source": [
    "# Parameters for the exponential distribution\n",
    "num_samples = 5\n",
    "\"\"\"kc12, kc21, spikeMax, Fnoise, scale, offset, nSpikes, kco1, koc2, ko12, ko21\"\"\"\n",
    "kc12 = tf.constant(0.1, dtype=tf.float32)  # Adjust this value as needed\n",
    "kc21 = tf.constant(0.1, dtype=tf.float32)  # Adjust this value as needed\n",
    "kco1 = tf.constant(0.01, dtype=tf.float32)\n",
    "koc2 = tf.constant(0.01, dtype=tf.float32)\n",
    "ko12 = tf.constant(0.01, dtype=tf.float32)\n",
    "ko21 = tf.constant(0.01, dtype=tf.float32)\n",
    "\n",
    "\n",
    "Anoise = tf.constant(.01, dtype=tf.float32)\n",
    "Fnoise = tf.constant(.01, dtype=tf.float32)\n",
    "SCALE = tf.constant(.25, dtype=tf.float32)\n",
    "#And an offset\n",
    "OFFSET = tf.constant(-0.1, dtype=tf.float32)\n",
    "num_spikes=10\n",
    "# nE = tf.constant(200, dtype=tf.int32) #number of events\n",
    "\n",
    "\n",
    "# Generate training data\n",
    "training_data = []\n",
    "lens=[]\n",
    "\"\"\"Actually replace \"Anoise\" with spikeMax later\"\"\"\n",
    "for sample in tqdm(range(num_samples)):   \n",
    "    params = tf.stack([kc12, kc21, Anoise, Fnoise, SCALE, OFFSET, num_spikes, kco1, koc2, ko12,ko21])  # Use tf.stack instead of tf.constant\n",
    "    segment = sim_channel(params)\n",
    "    lens.append(sum(abs(segment)))\n",
    "    training_data.append(segment)\n",
    "print(f\"Average duration was {sum(lens)/len(lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea8ea2-3420-4a42-8df7-f9361a049138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create REAL Trainging Data\n",
    "file_path = \"Lina2/4096lina11raw.csv\"\n",
    "df = pd.read_csv(file_path, header=None, names=[\"Raw\", \"Channels\"])\n",
    "df = df[[\"Channels\",\"Raw\"]]\n",
    "# now crop to just one phenotype. There seem multiple in this dataset.\n",
    "df=df[:75000]\n",
    "#df=df[:12000]\n",
    "df = pd.concat([df] * 5, ignore_index=True)\n",
    "noise = np.random.normal(0, 0.01, df[\"Raw\"].shape)\n",
    "df[\"Raw\"] += noise\n",
    "num_rows = (len(df) // size) * size\n",
    "print(num_rows)\n",
    "df = df.iloc[:num_rows]\n",
    "data_array = df.to_numpy()\n",
    "data_tensor = tf.convert_to_tensor(data_array, dtype=tf.float32)\n",
    "training_data = tf.reshape(data_tensor, [-1, size, 2])\n",
    "#Calculate real num_samples!\n",
    "num_samples= tf.shape(training_data)[0]\n",
    "#num_samples = 10 #debug\n",
    "#Only use windows where something happened!\n",
    "filterSilence = True\n",
    "if filterSilence:\n",
    "    first_column = training_data[:, :, 0]\n",
    "    all_same = tf.reduce_all(tf.equal(first_column, first_column[:, 0:1]), axis=1)\n",
    "    \n",
    "    # Filter out batches where all values in the first column are the same\n",
    "    training_data = tf.boolean_mask(training_data, ~all_same)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280dab93-b2e1-4214-b409-ca5f619e2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Raw\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fd338-3df9-4a62-91e1-f5a26648b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(data):\n",
    "    # Create a figure with two subplots (panels)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 6))\n",
    "     # Flatten the axs array for easy iteration\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for i in range(4):\n",
    "        axs[i].plot(data[i])\n",
    "        #axs[i].set_ylim([-200, 200])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotter(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a169f4-e9e4-4000-9a30-ba0653f109a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generator model\n",
    "\"\"\"kc12, kc21, spikeMax, Fnoise, SCALE, OFFSET, nSpikes, kco1, koc2, ko12, ko21\"\"\"\n",
    "gen_input_len=11\n",
    "def make_generator_model():\n",
    "    noise_input = tf.keras.layers.Input(shape=(gen_input_len,))\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(noise_input)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    \n",
    "    # Output layer without activation\n",
    "    raw_output = tf.keras.layers.Dense(gen_input_len)(x)\n",
    "    #Tickle the generator into the right ball park with the +10 etc...\n",
    "    # Apply appropriate activations/scaling to each output\n",
    "    #nE = tf.keras.layers.Lambda(lambda x: tf.abs(x) + 20)(raw_output[:, 0:1])  # Positive, non-zero\n",
    "    kc12= tf.keras.layers.Lambda(lambda x: tf.abs(x)+ 1e-6)(raw_output[:, 0:1])  # Positive, non-zero\n",
    "    kc21 = tf.keras.layers.Lambda(lambda x: tf.abs(x)+ 1e-3)(raw_output[:, 1:2])  # Positive, non-zero\n",
    "    #phase = tf.keras.layers.Lambda(lambda x: x * 2 * np.pi)(raw_output[:, 2:3])  # Any value, scaled to [0, 2π]\n",
    "    spikeMax = raw_output[:, 2:3] \n",
    "    Fnoise = tf.keras.layers.Lambda(lambda x: tf.abs(x))(raw_output[:, 3:4])  # Positive  \n",
    "    scale = tf.keras.layers.Lambda(lambda x: tf.abs(x)+0.1)(raw_output[:, 4:5])  # Positive  should it be?\n",
    "    offset = raw_output[:, 5:6]\n",
    "    nSpikes = tf.keras.layers.Lambda(lambda x: tf.round(x))(raw_output[:, 6:7])\n",
    "    kco1 = tf.keras.layers.Lambda(lambda x: tf.abs(x)+ 1e-3)(raw_output[:, 7:8])  # Positive, non-zero\n",
    "    koc2 = tf.keras.layers.Lambda(lambda x: tf.abs(x)+ 1e-6)(raw_output[:, 8:9])  # Positive, non-zero\n",
    "    ko12 = tf.keras.layers.Lambda(lambda x: tf.abs(x)+ 1e-3)(raw_output[:, 9:10])  # Positive, non-zero\n",
    "    ko21 = tf.keras.layers.Lambda(lambda x: tf.abs(x)+ 1e-6)(raw_output[:, 10:11])  # Positive, non-zero\n",
    "    \n",
    "    output = tf.keras.layers.Concatenate()([kc12, kc21, spikeMax, Fnoise, scale, offset, nSpikes, kco1, koc2, ko12, ko21])\n",
    "    #output = tf.keras.layers.Concatenate()([To, Tc])\n",
    "    #output = tf.keras.layers.Lambda(lambda x: x * 5)(output)  # Multiply by 10\n",
    "    \n",
    "    return tf.keras.Model(inputs=noise_input, outputs=output)\n",
    "\n",
    "\n",
    "# Define the discriminator model batch, record len, channels = events then noise\n",
    "num_points = T.numpy().item()\n",
    "def make_discriminator_model():\n",
    "    input_shape = (size,2) \n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Reshape input to add channel dimension\n",
    "    x = tf.keras.layers.Reshape((size, 2))(inputs)\n",
    "    \n",
    "    # 1D Convolutional layers\n",
    "    x = tf.keras.layers.Conv1D(8, kernel_size=5, strides=2, padding='same', activation='leaky_relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(16, kernel_size=5, strides=2, padding='same', activation='leaky_relu')(x)\n",
    "    x = tf.keras.layers.Conv1D(32, kernel_size=5, strides=2, padding='same', activation='leaky_relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3) (x)\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    \"\"\"\n",
    "    # Dense layers\n",
    "    x = tf.keras.layers.Dense(256, activation='leaky_relu')(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='leaky_relu')(x)\"\"\"\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = tf.keras.layers.Dense(1)(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# Loss functions and optimizers\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(fake_output), fake_output, from_logits=True))\n",
    "\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6ca5d-2ce2-44c3-b9c1-8be2c38ef6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function(experimental_compile=True)\n",
    "def train_step(real_data):\n",
    "    noise = tf.random.normal([batch_size, gen_input_len])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_params = generator(noise, training=True)\n",
    "\n",
    "        try:\n",
    "            generated_data = tf.map_fn(\n",
    "                sim_channel, \n",
    "                generated_params, \n",
    "                fn_output_signature=tf.float32,\n",
    "                parallel_iterations=1  # This can help with TensorArray issues\n",
    "            )\n",
    "            generated_data = tf.ensure_shape(generated_data, [batch_size, size, 2])\n",
    "        except Exception as e:\n",
    "            tf.print(\"sim_channel error:\", e)\n",
    "            return tf.constant(0, dtype=tf.float32), tf.constant(0, dtype=tf.float32)\n",
    "        \n",
    "        real_output = discriminator(real_data, training=True)\n",
    "        fake_output = discriminator(generated_data, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# Training loop\n",
    "@tf.function(experimental_compile=True)\n",
    "def train(dataset, epochs):\n",
    "    steps_per_epoch = math.floor(num_samples / batch_size)\n",
    "    #steps_per_epoch =2\n",
    "    tf.print(\"steps per epoch\", steps_per_epoch)\n",
    "    for epoch in range(getREALepoch(),epochs,1):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        read_set_lr()\n",
    "        # Initialize loss accumulators for each epoch\n",
    "        epoch_gen_loss = 0\n",
    "        epoch_disc_loss = 0\n",
    "        \n",
    "        for step, batch in tqdm(enumerate(dataset), total=steps_per_epoch, ncols=60):\n",
    "            #clear_output(wait=True)\n",
    "            if step >= steps_per_epoch:\n",
    "                break  # Move to the next epoch          \n",
    "            try:\n",
    "                gen_loss, disc_loss = train_step(batch)\n",
    "                #tf.print(\"gen_loss\",gen_loss)\n",
    "                epoch_gen_loss += gen_loss\n",
    "                #tf.print(\"epoch_gen_loss\",gen_loss)\n",
    "                epoch_disc_loss += disc_loss\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training: {e}\")\n",
    "                break\n",
    "\n",
    "        #tf.print(\"epoch_gen_loss\",epoch_gen_loss)\n",
    "        # Calculate average losses for the epoch\n",
    "        avg_gen_loss = epoch_gen_loss / steps_per_epoch\n",
    "        avg_disc_loss = epoch_disc_loss / steps_per_epoch\n",
    "        clear_output(wait=True)\n",
    "        tf.print(f\"Epoch {epoch + 1}/{epochs} - \"\n",
    "              f\"Generator Loss: {avg_gen_loss:.8f}, \"\n",
    "              f\"Discriminator Loss: {avg_disc_loss:.8f}\")\n",
    "        checkpoint.save(file_prefix = 'markovCheckpoints/checkpoint')\n",
    "        # Generate and plot sine waves\n",
    "        egs=2\n",
    "        \n",
    "        # Generate and plot data\n",
    "        EgNoise = tf.random.normal([egs, gen_input_len])\n",
    "        #print(\"EgNoise\", EgNoise[0])\n",
    "        generated_params = generator(EgNoise, training=False)\n",
    "        #print(steps_per_epoch)\n",
    "        # Define parameter names kc12, kc21, spikeMax, Fnoise, scale, offset, nSpikes, kco1, koc2, ko12, ko21\n",
    "        param_names = ['kc12', 'kc21', 'spikeMax', 'Fnoise', 'scale',\"offset\",\"nSpikes\", \"kco1\", \"koc2\",\"ko12\",\"ko21\"]\n",
    "        random_index1 = tf.random.uniform(shape=[], minval=0, maxval=egs-1, dtype=tf.int32)\n",
    "\n",
    "        params_list = generated_params[random_index1].numpy().tolist()\n",
    "        #Might be fun to collect these up to plot convergence if wanted?\n",
    "                \n",
    "        # Print each parameter with its name\n",
    "        with tf.io.gfile.GFile('output.csv', mode='a') as file:\n",
    "            # Check if the file is empty to write the header\n",
    "            if file.tell() == 0:\n",
    "                file.write(','.join(param_names) + '\\n')\n",
    "\n",
    "            for name, param in zip(param_names, params_list):\n",
    "                tf.print(f\"{name}: {round(param, 2)}|\", end = \" \")\n",
    "                file.write(f\"{name},{round(param, 2)}\\n\")\n",
    "         \n",
    "        gen_waves=[]\n",
    "        for i in range(egs):\n",
    "            gen_waves.append( sim_channel(generated_params[i]) )\n",
    "\n",
    "        \"\"\"\n",
    "        # Create a figure with two subplots (panels)\n",
    "        fig, axs = plt.subplots(egs, 1, figsize=(10, 6))\n",
    "        for i in range(egs):\n",
    "            axs[i].plot(gen_waves[i] )\n",
    "            #axs[i].set_ylim([0,1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        #tf.print(tf.shape(gen_waves))\n",
    "        random_index2 = tf.random.uniform(shape=[], minval=0, maxval=tf.shape(training_data)[0], dtype=tf.int32)\n",
    "\n",
    "        biPlotter([gen_waves[random_index1],training_data[random_index2]], random_index1, random_index2, epoch)\n",
    "              \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Completed {epoch + 1} epochs\")\n",
    "        if writeNow:\n",
    "            writeMe(epoch=epoch)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f5189-394e-4b9f-ac37-44adc8341983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biPlotter(data, n, m, epoch):\n",
    "    # Create a figure with two subplots (panels)\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 6))\n",
    "    axs[0].plot(data[0] )\n",
    "    axs[0].set_title(f\"Generated Wave, record {n}, epoch {epoch}\")\n",
    "    axs[0].set_ylim([-1,1.2])\n",
    "    axs[1].plot(data[1] )\n",
    "    axs[1].set_title(f\"Training Data, record {m}\")\n",
    "    axs[1].set_ylim([-1,1.2])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"chanFigs/fig{epoch}.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe44148-0908-4737-9dff-162920f8d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeMe(samples=100, dt=0.1, epoch=0, file=\"markovData/output.parquet\"):\n",
    "    sampleNoise = tf.random.normal([samples, gen_input_len])\n",
    "    generated_params = generator(sampleNoise, training=False)\n",
    "    gen_waves=[]\n",
    "    for i in range(samples):\n",
    "        gen_waves.extend( sim_channel(generated_params[i]) )\n",
    "    df = pd.DataFrame(gen_waves, columns=[\"Channels\", \"Noisy Current\"])\n",
    "    df[\"Time\"] = dt * pd.Series(range(len(df)))\n",
    "    df = df[[\"Time\", \"Channels\", \"Noisy Current\"]]\n",
    "    df.to_parquet(f\"{epoch}_{file}\")\n",
    "    print(f\"Data saved to {epoch}_{file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b7d3a-36e7-4b71-9bbf-f1c5236672ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getREALepoch() -> int:\n",
    "    import glob\n",
    "    import os\n",
    "    import re\n",
    "    \"\"\"\n",
    "    Save the current image to the working directory of the program.\n",
    "    \"\"\"\n",
    "    currentfiles = glob.glob(\"markovCheckpoints/*.index\")\n",
    "   \n",
    "    numList = [0]\n",
    "    for file in currentfiles:\n",
    "        i = os.path.splitext(file)[0]\n",
    "        try:\n",
    "            pattern = r'-(\\d+)'\n",
    "            num = re.findall(pattern, i)[0]\n",
    "            numList.append(int(num))\n",
    "        except IndexError:\n",
    "            pass\n",
    "    numList = sorted(numList)\n",
    "    return numList[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62630e-8587-4384-b04b-bb9dc76b296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    training_data).shuffle(5000).batch(batch_size, drop_remainder=True).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61330f0f-4fb5-449b-b2d9-bd9a94667257",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_compile=True)\n",
    "def read_set_lr():\n",
    "    with open(\"lr.txt\", \"r+\") as my_file:\n",
    "        data = my_file.read()\n",
    "        split = data.split('\\n')\n",
    "        parse_lr_from_file = lambda string: float(string.split(\":\")[1])\n",
    "        new_gen_lr = parse_lr_from_file(split[1])\n",
    "        new_disc_lr = parse_lr_from_file(split[2])\n",
    "        generator_optimizer.learning_rate.assign(tf.cast(new_gen_lr, generator_optimizer.learning_rate.dtype))\n",
    "        discriminator_optimizer.learning_rate.assign(tf.cast(new_disc_lr, discriminator_optimizer.learning_rate.dtype))\n",
    "        tf.print(f\"dLR: {discriminator_optimizer.learning_rate.value.numpy()},\\\n",
    "                    gLR: {generator_optimizer.learning_rate.value.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d3f1b7-2da8-4556-a693-a954734adc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_value = 1e-5\n",
    "\n",
    "# Convert the learning rate value to the appropriate dtype\n",
    "generator_optimizer.learning_rate.assign(tf.cast(learning_rate_value, generator_optimizer.learning_rate.dtype))\n",
    "learning_rate_value = 1e-4\n",
    "discriminator_optimizer.learning_rate.assign(tf.cast(learning_rate_value, discriminator_optimizer.learning_rate.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5465d-f070-48f7-a6c9-e797a95aca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_set_lr()\n",
    "discriminator_optimizer.learning_rate.value\n",
    "generator_optimizer.learning_rate.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b279af-95ef-4f33-8f86-f447e149662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"markovCheckpoints/checkpoints\"\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer, discriminator_optimizer=discriminator_optimizer, generator=generator, discriminator=discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6798b52-836f-4872-827d-3899b77e1cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writeNow=False\n",
    "epochs=20000\n",
    "train(training_dataset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4646c6-a93d-4ae6-bc63-cd3af97f53e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_value(generator_optimizer.learning_rate,1e-5)\n",
    "tf.keras.backend.set_value(discriminator_optimizer.learning_rate,1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b6205b-88e7-4f38-9dd6-def65ff0c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore('markovCheckpoints/checkpoint-3').assert_existing_objects_matched()\n",
    "\n",
    "tf.keras.backend.set_value(generator_optimizer.learning_rate,1e-5)\n",
    "tf.keras.backend.set_value(discriminator_optimizer.learning_rate,1e-4)\n",
    "\"\"\"gen_loss_list=[]\n",
    "disc_loss_list=[]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ec7c0-c563-4084-9199-f9f7cb38f9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537ed0e-dbd9-407e-b226-83236c9a92fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba7ba19-345d-4661-8e85-c0740ddef099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1bd28-58b0-4cfe-8882-493c43ceea28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
