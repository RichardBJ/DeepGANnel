{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff37764-5992-40b1-8127-f829f8fabf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 17:06:53.928196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-14 17:06:53.968796: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-14 17:06:53.981791: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-14 17:06:54.000004: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-14 17:06:55.070913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#Entirely untested. Like that'll work!\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "tf.config.run_functions_eagerly(True)\n",
    "size=1000\n",
    "parquet = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5edfa9f4-0706-4940-8b5e-70b9f9a51137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728922017.429324   24830 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728922017.473659   24830 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728922017.473740   24830 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728922017.479017   24830 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728922017.479174   24830 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728922017.479258   24830 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728922017.611167   24830 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728922017.611257   24830 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-14 17:06:57.611268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1728922017.611316   24830 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:21:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-14 17:06:57.611337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9702 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:21:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "dt = tf.constant(0.1, dtype=tf.float32)\n",
    "one = tf.constant(1, dtype=tf.float32)\n",
    "zero= tf.constant(0, dtype=tf.float32)\n",
    "pico = tf.constant(1e-12, dtype=tf.float32)\n",
    "T = tf.constant(size, dtype=tf.int32)  # In sample points :-)\n",
    "amplitude_noise_factor = tf.constant(5, dtype=tf.float32)\n",
    "#Must be a multiple of 2!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f36c0af-f3ee-4923-9e88-e5c7b9d73e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_compile=True)\n",
    "def tf_relaxation(binary_sequence, half_life=4.0, relaxation_amount=0.2):\n",
    "    \"\"\"\n",
    "    Apply exponential relaxation to a 1D binary sequence using vectorized TensorFlow operations.\n",
    "    Kind of works backward to what you might think.  Where amp i1 1 and relaxation 0f 0.5 is too 0.5 therefore\n",
    "    relaxation -0.5 is 0 to 1 but it then slowly rising to 1.5\n",
    "    Args:\n",
    "    binary_sequence: tf.Tensor, shape [time_steps], sequence of 0s and 1s\n",
    "    half_life: float, the half-life of the exponential decay\n",
    "    relaxation_amount: float, the amount of relaxation (positive or negative)\n",
    "    Kind of works backward to what you might think.  Where amp i1 1 and relaxation 0f 0.5 is too 0.5 therefore\n",
    "    relaxation -0.5 is 0 to 1 but it then slowly rising to 1.5\n",
    "    Returns:\n",
    "    tf.Tensor, shape [time_steps]\n",
    "    \"\"\"\n",
    "    # Convert input to float32\n",
    "    binary_sequence = tf.cast(binary_sequence, tf.float32)\n",
    "    \n",
    "    # Calculate decay rate\n",
    "    decay_rate = tf.math.log(2.0) / half_life\n",
    "    \n",
    "    # Find the indices where steps occur\n",
    "    steps = tf.not_equal(binary_sequence[1:] - binary_sequence[:-1], 0)\n",
    "    step_indices = tf.where(steps)[:, 0]\n",
    "    \n",
    "    # Calculate the time since each step\n",
    "    time_steps = tf.range(tf.shape(binary_sequence)[0], dtype=tf.float32)\n",
    "    time_since_step = time_steps[:, tf.newaxis] - tf.cast(step_indices, tf.float32)\n",
    "    \n",
    "    # Calculate the exponential decay for each step\n",
    "    decay = tf.exp(-decay_rate * tf.maximum(time_since_step, 0.0))\n",
    "    \n",
    "    # Calculate the relaxation effect\n",
    "    step_values = tf.gather(binary_sequence, step_indices + 1) - tf.gather(binary_sequence, step_indices)\n",
    "    relaxation_effect = relaxation_amount * step_values * decay\n",
    "    \n",
    "    # Sum the effects of all steps\n",
    "    total_relaxation = tf.reduce_sum(relaxation_effect, axis=1)\n",
    "    \n",
    "    # Add the relaxation to the original sequence\n",
    "    relaxed_sequence = binary_sequence + total_relaxation\n",
    "    \n",
    "    return relaxed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45183fe0-b457-435d-9e8b-00bb39720a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_compile=True)\n",
    "def normalize_row(row):\n",
    "    # Find non-zero elements\n",
    "    non_zero_mask = tf.not_equal(row, 0.0)\n",
    "    non_zero_sum = tf.reduce_sum(tf.boolean_mask(row, non_zero_mask))\n",
    "    \n",
    "    # Normalize only non-zero elements\n",
    "    normalized_row = tf.where(\n",
    "        non_zero_mask,\n",
    "        row / non_zero_sum,\n",
    "        row\n",
    "    )      \n",
    "    return normalized_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15cd5fea-670c-4dec-bb8b-2d265bc9f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is only going to work properly with n = 1 and it's slow.\n",
    "@tf.function(experimental_compile=True)\n",
    "def sim_channel(params, num_steps=T, n=1):\n",
    "    pc12, pc21, relaxation, Fnoise, scale, offset, relaxT, pco1, poc2, po12, po21 = params\n",
    "    \n",
    "    unnormalized_matrix = tf.stack([\n",
    "         #s0     #s1  #s2   #s3\n",
    "        [1-pc12, pc12, zero,   zero],          # state 0 CLOSED\n",
    "        [pc21, 1-pc21-pco1, pco1, zero],   # state 1 CLOSED\n",
    "        [zero, poc2, 1-poc2-po12, po12],   # state 2 OPEN\n",
    "        [zero,    zero,   po21, 1-po21]        # state 3 OPEN\n",
    "    ])\n",
    "    # tf.random.set_seed(int(time.time()) )\n",
    "    # Normalize each row while preserving zero probabilities\n",
    "    transition_matrix = tf.map_fn(normalize_row, unnormalized_matrix)\n",
    "\n",
    "    initial_distribution = tfp.distributions.Categorical(probs=[0.3, 0.3, 0.2, 0.2])\n",
    "    for i in range(n):\n",
    "        markov_chain = tfp.distributions.MarkovChain(\n",
    "            initial_state_prior=initial_distribution,\n",
    "            transition_fn=lambda _, state: tfp.distributions.Categorical(probs=tf.gather(transition_matrix, state)),\n",
    "            num_steps=num_steps\n",
    "        )\n",
    "        states = markov_chain.sample()\n",
    "        #Assuming we have one channel so emission is...\n",
    "        if i==0:\n",
    "            emissions = tf.where(tf.less(states, 2), tf.zeros_like(states), tf.ones_like(states))\n",
    "        else:\n",
    "            emissions = emissions + tf.where(tf.less(states, 2), tf.zeros_like(states), tf.ones_like(states))\n",
    "\n",
    "    channels = tf.cast(emissions, dtype=tf.float32)\n",
    "    # Generate pink noise\n",
    "    white_noise = tf.random.normal(shape=[T])\n",
    "    fft_len = T // 2 + 1\n",
    "    f = tf.range(1, fft_len, dtype=tf.float32)\n",
    "    spectrum = 1.0 / tf.sqrt(f)\n",
    "    spectrum = tf.concat([tf.constant([1.0]), spectrum], axis=0)\n",
    "    white_noise_fft = tf.signal.rfft(white_noise)\n",
    "    pink_noise_fft = white_noise_fft * tf.cast(spectrum, tf.complex64)\n",
    "    pink_noise = tf.signal.irfft(pink_noise_fft)\n",
    "    pink_noise -= tf.reduce_mean(pink_noise)\n",
    "    pink_noise = pink_noise / tf.math.reduce_std(pink_noise)\n",
    "    \n",
    "    # Scale noise based on signal amplitude: Problem is if all same level!!\n",
    "    signal_amplitude = channels * scale\n",
    "    normalized_amplitude = (signal_amplitude - tf.reduce_min(signal_amplitude)) / (tf.reduce_max(one+signal_amplitude) - (tf.reduce_min(signal_amplitude)+pico))\n",
    "    if tf.reduce_min(normalized_amplitude) == tf.reduce_max(normalized_amplitude):\n",
    "        # If it's all the same level make it all = to 1, but will still look funny if all open!\n",
    "        amplitude_factor = one + zero*normalized_amplitude\n",
    "    else:\n",
    "        amplitude_factor = one + amplitude_noise_factor * normalized_amplitude\n",
    "    \n",
    "    noise = pink_noise * Fnoise * amplitude_factor\n",
    "\n",
    "    # Add relaxation\n",
    "    modified_raw_column = (channels * scale) + offset\n",
    "    modified_raw_column = tf_relaxation(modified_raw_column, half_life=relaxT, relaxation_amount=relaxation)\n",
    "\n",
    "    modified_raw_column += noise\n",
    "    \n",
    "    # Combine channels and modified raw column\n",
    "    image = tf.stack([channels, modified_raw_column], axis=1)\n",
    "    \n",
    "    # Final safeguard against NaN values\n",
    "    image = tf.where(tf.math.is_nan(image), tf.zeros_like(image), image)\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b759cb-7340-47f2-a241-346b3f8e145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████▌   | 9242/10000 [66:40:51<5:28:08, 25.97s/it]\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the exponential distribution\n",
    "num_samples = 10000\n",
    "nchan = 3\n",
    "saveSim=True\n",
    "\"\"\"pc12, pc21, relaxation, Fnoise, scale, offset, relaxT, pco1, poc2, po12, po21\n",
    "         #s0     #s1  #s2   #s3\n",
    "        [1-pc12, pc12, zero,   zero],          # state 0 CLOSED\n",
    "        [pc21, 1-pc21-pco1, pco1, zero],   # state 1 CLOSED\n",
    "        [zero, poc2, 1-poc2-po12, po12],   # state 2 OPEN\n",
    "        [zero,    zero,   po21, 1-po21]        # state 3 OPEN\n",
    "\"\"\"\n",
    "pc12 = tf.constant(.01, dtype=tf.float32)  # Decreased from 0.1\n",
    "pc21 = tf.constant(.8, dtype=tf.float32)  # Decreased from 0.1\n",
    "pco1 = tf.constant(0.1, dtype=tf.float32)  # Decreased from 0.01\n",
    "poc2 = tf.constant(1e-2, dtype=tf.float32)  # Decreased from 0.0001\n",
    "po12 = tf.constant(0.1, dtype=tf.float32)  # Decreased from 0.01\n",
    "po21 = tf.constant(1e-2, dtype=tf.float32)\n",
    "relaxation = tf.constant(0.02, dtype=tf.float32)\n",
    "Fnoise = tf.constant(.04, dtype=tf.float32)\n",
    "SCALE = tf.constant(.6, dtype=tf.float32)\n",
    "#And an offset\n",
    "OFFSET = tf.constant(-0.4, dtype=tf.float32)\n",
    "relaxT=150\n",
    "\n",
    "def within_x_percent(a, b, x):\n",
    "    return abs(a - b) <= (x / 100) * max(a, b)\n",
    "\n",
    "# Generate training data\n",
    "training_data = []\n",
    "saver=[]\n",
    "\n",
    "first=True\n",
    "for sample in tqdm(range(num_samples)):   \n",
    "    params = tf.stack([pc12, pc21, relaxation, Fnoise, SCALE, OFFSET, relaxT, pco1, poc2, po12, po21])  # Use tf.stack instead of tf.constant\n",
    "    segment = sim_channel(params, n=nchan)\n",
    "    training_data.append(segment)\n",
    "    if saveSim:\n",
    "        if first:\n",
    "            saver.extend(segment)\n",
    "            first=False\n",
    "            s=1\n",
    "        else:\n",
    "            laststate = np.round(np.asarray(saver)[-1,0])\n",
    "            newstate = np.round(np.asarray(segment)[0,0])\n",
    "            \"\"\"only append if first point is close to last point of previous segment!\"\"\"\n",
    "            if within_x_percent(newstate, laststate, 20):\n",
    "                saver.extend(segment)\n",
    "                s=s+1\n",
    "if saveSim:\n",
    "    time_col = np.arange(0, len(saver) * dt, dt)\n",
    "    np_data = np.column_stack((time_col, np.asarray(saver) ))\n",
    "    #May have to untensorflow this output\n",
    "    # Convert to a pandas DataFrame\n",
    "    df = pd.DataFrame(np_data, columns=['Time', \n",
    "                                              'Channels', \n",
    "                                              'Noisy Current'])\n",
    "    if parquet:\n",
    "        df.to_parquet(\"regularSimv103OffyB.parquet\", index=False)\n",
    "    else:\n",
    "        df.to_csv(\"regularSimv103OffyB.csv\", index=False)\n",
    "print(f\"Number of pages collected = {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba2bc1-ae21-41b7-996f-a609ff0f298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(data):\n",
    "    # Create a figure with two subplots (panels)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 6))\n",
    "     # Flatten the axs array for easy iteration\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for i in range(4):\n",
    "        axs[i].plot(data[i])\n",
    "        axs[i].set_ylim([-1,5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotter(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6014df-095a-4178-80bb-f6cc7f510301",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
